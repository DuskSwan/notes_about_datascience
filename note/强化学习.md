[toc]



# 思路

## 基本概念

强化学习 (Reinforcement learning, RL) 讨论的问题是，一个智能体 (agent) 如何在复杂的环境(environment) 中行动，来达成其目的。每一轮行动中，智能体要感知环境的状态 (state) 或者说获取环境的观察值 (observation)，采取一个动作 (action)，然后获得相应奖励 (reward)。一个好的策略能够使得智能体最终获得较高的奖励，学习这个好策略的过程就是强化学习。

在训练强化学习智能体时，往往要让它反复进行游戏。从开始到结束的一局游戏，有时也称为一个回合 (episode)。前述的概念中，奖励有时也称为回报、评价、分数。

用数学语言定义强化学习的过程：

我们的智能体agent面对着$X$这一状态空间，每个$s\in X$是一个状态；可采取的全部行为构成则空间$A$，每个$a\in A$是一个可采取的行为。在状态$s$处采取行为$a$，可能会导致多种新状态出现，假设出现状态$y\ (y\in X)$的概率为$p$，那么函数$P: X\times A\times X\mapsto \mathbb{R},P(x,a,y)=p$就描述了所有可能的状态转移概率。为了评价一次转移的好坏，定义奖励函数$R:X\times A\times X\mapsto \mathbb{R}$来描述”状态$x$下采取行为$a$转移到状态$y$“的得分。

上述的$(X,A,P,R)$确定下来后，agent所面对的环境就完全确定了，其任何行为都会有准确的评分。这样的环境$E=(X,A,P,R)$下，我们希望机器给出一个策略 (policy) $\pi$。策略有两种形式，确定性策略是函数$\pi:X\mapsto A,\pi(x)=a$，表示遇到状态$x$就采用行为$a$；随机性策略则是函数$\pi:X\times A\mapsto \mathbb{R},\pi(x,a)=p$，表示在状态$x$下有$p$的概率选择行为$a$，这里还要求有$\sum_a\pi(x,a)=1$。

为了评价一个策略的好坏，考虑长期执行策略后的累计奖励。常用的价值函数有两种，一是状态价值 (state-value) 函数$V_\pi(s)$，表示从某个状态$s$开始，遵循策略$\pi$所能获得的预期累积奖励；二是动作价值 (action-value) 函数$Q_\pi(s,a)$，表示在状态$s$下采取动作$a$，然后遵循某个策略$\pi$所能获得的预期累积奖励。

求解一个使得价值函数最大的策略$\pi$，就是强化学习的最终目标。



## 分类

可以按照多种角度来对强化学习方法做分类。

基于模型的 (Model-based) VS 无模型 (Model-free)：如果环境$E=(X,A,P,R)$能显式写出，或者能用模型近似出，那么这就是一个建立随机过程上的最优化问题，可以求解析解，这就是基于模型的强化学习。反之如果智能体无需显式地构建环境模型，直接从与环境的交互中学习最优策略，就是无模型的。

基于策略 (Policy-based) VS 基于价值 (Value-based) VS Actor-Critic：基于策略指的是直接学习出策略，每次根据状态给出下一步动作的概率；基于价值则是学习一个价值函数（例如$Q$函数），然后根据这个价值函数来选择动作（选择使$Q$值最大的动作）；Actor-Critic结合了二者，Actor (行动者) 负责学习策略并选择动作，Critic (评论者) 负责学习价值函数并评估Actor选择的动作，同时Critic的评估结果用于指导Actor改进策略。

在线策略  (On-Policy) VS 离线策略 (Off-Policy)：在线策略指agent一边应用策略交互，一边学习（更新）这个策略；离线策略则可以使用与要学习的策略相异的策略，”使用的策略“和”学习的策略“可以有关也可以无关，甚至可以先交互一通，事后再根据采集的数据学习。

# 经典算法

## Q-Learning

Q-Learning是无模型的、基于价值的、离线的策略。其思路是建立一个Q值表，这个表描述了一个离散函数$Q:(X,A)\mapsto \mathbb{R}$，也即对每个状态$s\in X$，$Q(s,a)$描述了此时采取动作$a$的收益。有了这个表，每次行动时只要选取使得Q值最大的动作即可。

建立这个表的方式是迭代。先让Q值表随机初始化，比如初始全部为$0$，令agent以任何策略行动一番，收集数据，用收集到的数据更新Q值表，最终即可收敛到一个真实有效的Q值表。

在描述Q值表的迭代过程之前，还要考虑一个问题，为什么在$s$处不直接采用使得单步收益最大的动作？这显然是贪心策略，而贪心容易陷入局部最优解，错过全局最优解。因此建立Q值表时不仅要考虑当前一步的收益，还要考虑这一步带来的未来收益，需要一个参数控制“对未来收益的关注程度”，这个参数就记为$\gamma$，称折扣因子，取值范围在 $[0,1]$ 之间。它衡量了未来奖励的重要性。$\gamma$ 接近 $0$ 意味着智能体更关注即时奖励；$\gamma$ 接近 $1$ 意味着智能体更关注长期奖励。

Q值表的迭代公式如下：
$$
Q(s_t,a_t) = Q(s_t,a_t)+α[r_{t+1}+γ\max_{a'}Q(s_{t+1},a')−Q(s_t,a_t)]
$$
$t$表示第$t$步更新，$\alpha$是学习率，这都是迭代公式中的常见部分。这一公式表示每次将$(s_t,a_t)$处的收益值$Q(s_t,a_t)$向着$r_{t+1}+γ\max_{a'}Q(s_{t+1},a')$方向移动，后者代表更加”正确“的收益值。其中$r_{t+1}$是在状态 $s_t$ 下执行动作 $a_t$ 后，单步转移的奖励；而$\max_{a'}Q(s_{t+1},a')$表示下一状态 $s_{t+1}$ 的最大可能Q值，二者相加正衡量了当前$(s_t,a_t)$处的收益值。

> 顺带一提，$r_{t+1}+γ\max_{a'}Q(s_{t+1},a')−Q(s_t,a_t)$这部分被称为时序差分误差。不知道为什么要给它起个名字。

前面我们说的是”令agent以任何策略行动一番“，但实际上这个策略也不能完全随机，不然Q值表将无法收敛，或是收敛需要的时间极长。现实中采用的是$\epsilon$-贪婪 (greedy) 策略。

在行动时，用概率值$\epsilon$表示探索的概率，每一步中，以$\epsilon_t$的概率执行探索——随机选择一个动作，以$1-\epsilon_t$的概率按照Q值表执行最佳动作。$\epsilon_t$初始是$1$，也即在Q值表未建立起来时随机选动作，而后随着$t$逐渐减小到$0$。这种减小可以是线性的，也可以是指数的，如
$$
ϵ(t)=ϵ_{end}+(ϵ_{start}−ϵ_{end})e^{−λt}
$$
$\lambda$是控制速度的参数，是一个很小的数。

进行多局游戏，每一句游戏进行多步，游戏结束或者达到最大步数时停止。在这样的反复迭代中就建立了完整的Q值表。

一个应用Q-learning的典型例子是迷宫找路。

对于状态空间或动作空间非常大的问题，Q表会变得极其巨大，无法存储和计算。这是Q-learning面临的最大挑战。另外Q表要求离散的状态和动作，无法直接处理连续值。

## SARSA

SARSA 这个名字本身就概括了它的核心更新过程：它需要当前状态 State、当前动作 Action、获得的 Reward、下一个状态 State'、以及下一个动作 Action'。

SARSA 和 Q-learning 非常相似，它们都是免模型（model-free）、基于价值（value-based）的时序差分（Temporal Difference, TD）控制算法。然而，它们之间有一个关键的区别，这个区别决定了它们是“在线策略”还是“离线策略”算法。

Q-learning执行的是$\epsilon$-贪婪 (greedy) 策略，学习目标则是Q值表（决定的最佳行动策略），二者并不完全相同。SARSA学习的则是在所遵循的特定策略（通常就是 $ϵ$-贪婪策略）下，每个（状态，动作）对的价值。

SARSA的迭代公式如下：
$$
Q(s_t,a_t) = Q(s_t,a_t)+α[r_{t+1}+γQ(s_{t+1},a_{t+1})−Q(s_t,a_t)]
$$
与 Q-learning 的区别仅仅在于， Q-learning 使用 $\max_{a'}Q(s_{t+1},a')$（下一步最优动作的奖励值），而 SARSA 使用 $Q(s_{t+1},a_{t+1})$（下一步实际动作的奖励值）。

这个简单的差异带来的影响是，SARSA会更倾向于“安全”的路径，因为它会考虑探索带来的负面奖励。而Q-learning可以学到最短路径，即使这条路很危险。不妨想象一个网格世界，智能体从起点开始，目标是到达终点。在终点和起点之间有一条“悬崖”，掉入悬崖会受到巨大的负奖励（例如 −100），并回到起点。这时Q-learning会找到到达终点的最短路径，即使这意味着它需要沿着悬崖的边缘行走；而SARSA会倾向于选择一条更远离悬崖的“安全”路径，即使这条路径可能更长。

与 Q-learning 相同，SARSA不适用于状态空间或动作空间非常大的问题，因为需要一个Q表；并且同样需要离散的状态和动作。在某些问题上，SARSA 可能需要更多的时间来收敛，”在线“的性质使得收敛不太容易。

## 深度Q网络（Deep Q-Networks, DQN）

Q-learning无法解决连续型状态或者说状态空间非常大的局面，DQN 的出现正是为了解决 Q-learning 的这个核心局限性，其核心思想是使用一个深度神经网络来近似函数 $Q(s,a)$。

对于参数为$\theta$的网络，自然地，我们考虑用神经网络的迭代公式
$$
\theta_{t+1} = \theta_t + \alpha\nabla_\theta(r_{t+1}+γ\max_{a'}Q(s_{t+1},a')−Q(s_t,a_t))
$$
来更新，但这样存在问题，一是连续的时间步中通常会经历高度相关的状态序列（例如，在游戏中，相邻帧的图像非常相似），差分值极小，用梯度下降法难以收敛；二是差分值中包含了当前的 Q 网络本身，这意味着差分值会随着 Q 网络的更新而不断变化，导致训练目标不稳定。

为了解决这些稳定性问题，DQN 引入了两个关键的技术，经验回放和目标网络固定。

经验回放 (Experience Replay) 指的是，智能体与环境交互的经验（也即$(s_t,a_t,r_{t+1},s_{t+1})$，将其作为一个样本）不再立即用于训练。相反，这些经验被存储在一个名为经验回放缓冲区（Replay Buffer）的大型数据库中。在训练时，不再按顺序使用最新的经验，而是从回放缓冲区中随机抽取一个批次（mini-batch）的经验来更新 Q 网络。

目标网络固定指的是，使用两个相同的神经网络。一个称为当前 Q 网络 (Current Q-network) 或在线网络 (Online Network)，用于计算当前的 Q 值 $Q(s_t,a_t)$ 和选择动作。另一个称为目标 Q 网络 (Target Q-network)，用于计算 Q-learning 更新公式中的目标值 $\max_{a'}Q(s_{t+1},a')$​。当前 Q 网络在每个训练步都会更新其权重。目标 Q 网络的权重则定期地、缓慢地（例如，每隔 C 个训练步，将当前 Q 网络的权重复制到目标 Q 网络）进行更新。

由于计算差分值用的是暂时不变的目标 Q 网络，所以差分值不会一直很小，当前Q网络也不会时刻影响差分值。这就解决了前述的问题。

训练时，每一局（episode）游戏中都会进行多步，每一步（step）执行的操作包括：

- 以$\epsilon$的概率随机行动，以$1-\epsilon$的概率选择当前Q网络认为的最佳动作作为$a_t$；
- 根据动作$a_t$计算单步奖励$r_{t+1}$，和新状态$s_{t+1}$；
- 将经验元组（样本） $(s_t,a_t,r_{t+1},s_{t+1})$ 存储到经验回放缓冲区 D 中；
- 训练当前 Q 网络（每步都进行）：
  - 从经验回放缓冲区 D 中随机抽取一个批次的样本 $(s_j,a_j,r_j,s_j')$，计算Q值误差$y_j=r_j+\max_{a'}Q_t(s_j',a')-Q_c(s_j,a_j)$（注意这里的$Q_t$表示目标Q网络，$Q_c$表示当前Q网络）；
  - 用误差值来更新当前Q网络的参数，比如用MSE做损失函数，则损失函数就是$L=\frac1b\sum y_j^2$，其中$b$是批次大小；（也可以理解成，$r_j+\max_{a'}Q_t(s_j',a')$是”真实值“，$Q_c(s_j,a_j)$是预测值，据此算损失函数。）
- 更新目标Q网络（每隔$C$步）：将目标Q网络的参数更新为当前Q网络的。

## REINFORCE

REINFORCE全称为"REward Increment = Nonnegative Factor × Offsetted Reward Characteristic Explanation"（这名字显然是先射箭后画靶……），是最早也是最经典的策略梯度（分类中“基于策略的”）算法。它的核心思想是通过一个完整的“回合”（episode）中得到的总回报，来更新策略网络，使那些带来高回报的动作被选中的概率增加，而带来低回报的动作被选中的概率减少。

假设$π_θ$是由参数$ θ $定义的策略，它决定了在每个状态下采取动作的概率，换言之$\pi_\theta(a|s)$是一个在状态$s$的条件下、关于行动$a$的函数。我们希望这个策略“好”，“好”的标准应该是，在此策略指导下的收益期望值高。

用$\tau$表示一系列状态、行为、回报组成的轨迹，也即$\tau=\{(s_i,a_i,r_i)\}_{i=0}^T$（或者$\tau=\{r(s_i,a_i)\}_{i=0}^T$，Gemini和GPT用的记法不一样，我采用前者）。使用策略$\pi_{\theta}$时，由于每次采取动作有一个概率，所以可能产生多条轨迹。在任一轨迹$\tau$下，总收益可以写作
$$
G(\tau) = \sum_{i=0}^{T} r_{i}
$$
那么策略$\pi_\theta$带来的期望收益就是
$$
J(\theta) 
= \mathbb{E}_{\tau\sim\pi_\theta}(G(\tau))
= \mathbb{E}_{\tau\sim\pi_\theta} \left( \sum_{i=0}^{T} r_{i} \right)
$$
如果我们能写出这个函数的表达式，然后求导，就可以用梯度下降法来求极大值，从而找到了最好的$\theta$，确定最佳策略。

相信你也能看出，这个求导写不出来。幸运的是，策略梯度定理（Policy Gradient Theorem）给出了如何计算目标函数$ J(θ) $的梯度 $∇_θJ(θ)$，这样一来还是可以用梯度下降法求最优解。

可以证明：
$$
\nabla_\theta J(\theta)
  = \mathbb{E}_{\tau\sim\pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t)\,G_t\right]
$$
其中 $G_t=\sum_{k=t}^T r_t$ 是从时刻 $t$ 开始的总回报。用采样求均值的方式来近似期望，这个梯度现在就可以算出来了。

简单说一下对式子的理解，$\nabla_\theta \log \pi_\theta(a_t|s_t)$是$\pi_\theta(a_t|s_t)$的对数似然梯度（log-likelihood gradient）。在第$t$步，按照策略采用动作$a_t$，如果这个动作好则$G_t$会是正数，向着$\nabla_\theta \log \pi_\theta(a_t|s_t)$方向移动参数$\theta$会导致$\pi_\theta(a_t|s_t)$增大，又增大了采取动作$a_t$的概率；反之，如果动作$a_t$不好，会导致$\theta$向着对数似然梯度的反方向移动，让该动作出现的概率变低。这里的$G_t$只考虑从第$t$步开始的收益，因为之前的奖励对当前步的决定不产生影响。用$G_t$来对对数似然梯度加权，得到就是“平均来看”合适的对数似然梯度。

再打个补丁，将$G_t=\sum_{k=t}^T r_t$修改成$G_t=\sum_{k=t}^T \gamma^{k-t}r_t$会更好，其中$ γ\in[0,1) $是折扣因子。这是因为，考虑价值时首先要考虑下一步立刻兑现的价值，未来的收益当然也要考虑，但是距离越远权重越小，所以才会有折扣因子来调节。

> 说明一下，REINFORCE 算法由 Ronald J. Williams 在 1992 年提出，他提出了一个与策略梯度定理非常相似的、可以用于计算策略梯度的数学表达式。但是策略梯度定理作为正式的定理，通常被归功于 Sutton 等人在 1999 年的工作，他们将这个思想形式化为一个更通用的定理。策略梯度定理的正式形式为
> $$
> \nabla_\theta J(\theta)
>   = \mathbb{E}_{s,a}\left[ \nabla_\theta \log \pi_\theta(a|s)\,Q_\pi(s,a)\right]
> $$
> 其中$Q_\pi(s,a)$是“依照策略$\pi$时，在状态$s_t$下采取行动$a_t$所产生的价值”（就是本文最开始，基本概念中的状态价值函数，我自己都忘了，晕）。可以看出，该形式是REINFORCE中迭代形式的推广。

与一般的优化问题不太一样，现在要求的是使得期望收益最大的解，所以不是往梯度下降方向移动，而是梯度上升。算法的流程为：

1. 初始化：随机初始化策略网络的参数 θ。
2. 生成轨迹：使用当前策略 $π_θ$ 与环境进行交互，直到一个回合结束。收集完整的轨迹 $τ=(s_0,a_0,r_0,s_1,a_1,r_1,...,s_T,a_T,r_T)$。
3. 计算回报：对于轨迹中的每一个时间步 $t=0,1,...,T−1$，计算从该时间步开始的总回报 $G_t=∑_{k=t}^{T}γ^{k−t}r_{k}$。
4. 计算梯度：使用收集到的轨迹和计算出的回报，来近似策略梯度。在实践中，通常将梯度计算转化为一个损失函数，然后对损失函数求导。比如定义$ L(θ)=-∑_{t=0}^T\log π_θ(a_t∣s_t)G_t$，对这个损失函数求导，就可以得到梯度$-\nabla_\theta \log \pi_\theta(a_t|s_t)\,G_t$，最小化这个损失函数，就等价于最大化期望回报。
5. 更新策略：使用梯度下降法更新策略网络的参数 $θ$，即$θ←θ−α∇_θL(θ)$
6. 重复：重复步骤2-5，直到策略收敛。

现在的算法存在一个问题，即便在同一策略下，不同轨迹之间的差距也可能很大，换言之$\tau$的方差很大，导致$G_t$的方差大，从而计算出的梯度很不稳定，算法难以收敛。为了解决这个问题，研究者们提出了基线（Baseline）的概念。通过从回报 $G_t$ 中减去一个基线 $b(s_t)$，来使得$G_t$更稳定。此时梯度公式变成：
$$
\nabla_\theta J(\theta)
  = \mathbb{E}_{\tau\sim\pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t)(G_t-b(s_t))\right]
$$
由于基线函数中不涉及$\theta$，所以不影响求导结果（期望），但是可以使得方差变小。这个修正过的$G_t-b$称为优势函数。

从减小方差的角度，最朴素的基线就是一个常数，稍微复杂一点的是$G_t$和常数做加权平均。

推广一下，我们定义状态价值函数（State-Value Function）$V(s)$来描述一个状态的价值，从高价值的状态出发得到的$G_t$会比较高，所以用$G_t-V(s_t)$来作为新的优势函数。那么如何确定$V$呢？我们立刻会想到用一个网络去拟合，这其实已经步入了Actor-Critic框架，将在后文中详细介绍。

## A2C (Advantage Actor-Critic)

Actor-Critic 的核心思想是用两个独立的神经网络来协同工作，一个作为 Actor，另一个作为 Critic。Actor学习出一个策略，负责在给定状态下选择动作，它是一个策略网络$ π_θ(a|s)$，其输出是每个动作的概率分布。Critic学习一个状态价值函数（value function），负责评估 Actor 的表现，它是一个价值网络 $V(s)$，其输出是给定状态的期望回报。Actor 会利用 Critic 的评价来更新自己的策略，而 Critic 则利用环境的奖励和 Actor 的行动来更新自己的评价能力。

具体说到A2C，该算法的核心思路和REINFORCE是一样的，都是利用策略梯度定理来优化策略函数，A2C的优势函数是一个更稳健高效的$A(s_t,a_t)$。在此我们会完整地介绍A2C的思路，即便抛开REINFORCE部分的知识也依然可以完全理解。（因此REINFORCE中定义过的记号会重申一遍╮(╯▽╰)╭）

假设$π_θ$是由参数$ θ $决定的策略，它是一个函数$\pi_\theta(a|s)$，表示在状态$s$的条件下、采取行动$a$的概率为$\pi_\theta(a|s)$。使用一个给定的策略$\pi_\theta$，会产生一系列状态、行为、回报三元组，这个序列记为$\tau=\{(s_t,a_t,r_t\}_{t=0}^T$，其中$r_t$是在第$t$步中，面对状态$s_t$采取行动$a_t$获得的即时奖励。这样的序列服从一个特定的、被$\pi$决定的分布，表示为$\tau\sim\pi_\theta$。

我们想要评价一个策略，于是设计一个得分来衡量策略的好坏。又考虑到序列$\tau$是策略$\pi_\theta$的实例，所以可以先设计关于$\tau$的分数，对其取期望就代表了$\pi_\theta$的分数。

关于$\tau$的分数被定义成$\sum_{t=0}^T\gamma^tr_t$，其中$\gamma\in[0,1)$，称为折扣因子，表达了对未来奖励的关心程度。现在，对策略$\pi_\theta$的评分就定义为
$$
J(\theta) = \mathbb{E} _{\tau\sim\pi_\theta} \left[ \sum_{t=0}^T\gamma^tr_t \right]
$$
如果能对这个式子求导，就能优化$\theta$，就能求出最佳策略$\pi_\theta$了，而策略梯度定理恰恰保证了这一点可以做到。为了描述策略梯度定理，先定义一些概念。

首先是第$t$步的折现回报（discount return）$G_t=\sum_{k=t}^T \gamma^{k-t}r_t=r_t+\gamma r_{t+1}+\gamma^2t_{t+2}+\cdots$，它指的是从时间步$ t $开始，到整局游戏结束为止的所有未来奖励的折现总和。

接下来是动作价值函数$Q_\pi(s_t,a_t)$，显然有$Q_\pi(s_t,a_t)=\mathbb{E} _{\tau\sim\pi}(G_t|s_t,a_t)$，它描述的是第$t$步时面对状态$s_t$时采取动作$a_t$的价值，这个价值里还包含了未来潜在的收益。

最后是状态价值函数$V_\pi(s_t)=\mathbb{E} _{a_t}(Q_\pi(s_t,a_t)|s_t)=\mathbb{E} _{\tau\sim\pi}(G_t|s_t)$，描述的是状态$s_t$的“平均”价值。

有了两个价值函数，可以描述策略梯度定理了，它的表达式为：
$$
\nabla_\theta J(\theta)
=   \mathbb{E}_{s,a}\left[\nabla_\theta \log \pi_\theta(a|s)\,Q_\pi(s,a) \right]
$$

> 策略梯度定理的证明暂且略过。

实际计算中，$Q_\pi(s,a)$的值可以采样$\tau$来估计，其方差较大，导致$\nabla_\theta \log \pi_\theta(a|s)\,Q_\pi(s,a)$方差也大，用策略梯度定理估计出来的梯度很不稳定。于是设法降低方差。幸运的是，又可以证明
$$
\nabla_\theta J(\theta)
=   \mathbb{E}_{s,a} \Big[\nabla_\theta \log \pi_\theta(a|s)\, \big(Q_\pi(s,a)-g(s)\big) \Big]
$$
其中$g(s)$是任意只与$s$有关的函数，不依赖动作$a$使得它不影响期望计算。于是，我们可以用$Q_\pi(s,a)-g(s)$取代$Q_\pi(s)$来降低方差，这样的$g(s)$称为基线函数，该式子中的$Q_\pi(s,a)-g(s)$称为优势函数。

> 这个证明利用了
> $$
> \sum_{a}\pi_\theta(a\mid s)\,\nabla_\theta\ln\pi_\theta(a\mid s)
> = \sum_a \nabla_\theta\pi_\theta(a\mid s)
> = \nabla_\theta\Bigl(\sum_a \pi_\theta(a\mid s)\Bigr)
> = \nabla_\theta 1
> = 0,
> $$
> （该证明来自ChatGPT）

可以证明，在所有的$g(s)$中，使得$\nabla_\theta \log \pi_\theta(a|s)\, \big(Q_\pi(s,a)-g(s)\big)$方差最小的恰为$V_\pi(s)$，记$A(s,a)=Q_\pi(s,a)-V_\pi(s)$，于是我们实际采用的计算梯度的式子是
$$
\nabla_\theta J(\theta)
=   \mathbb{E}_{s,a} \Big[\nabla_\theta \log \pi_\theta(a|s)\, A(s,a) \Big]
$$

> 要证明这个，可以直接写出$\nabla_\theta \log \pi_\theta(a|s)\, \big(Q_\pi(s,a)-g(s)\big)$​的方差，可计算得方差最小时有
> $$
> g^*(s)
> = \frac{\mathbb{E}[Q_\pi(s,a)]\;\mathbb{E}[\|g\|^2]}{\mathbb{E}[\|g\|^2]}
> = \mathbb{E}_{a\sim\pi(\cdot|s)}\bigl[Q_\pi(s,a)\bigr]
> = V_\pi(s).
> $$
> （该证明来自ChatGPT）

优势函数$A(s,a)=Q_\pi(s,a)-V_\pi(s)$衡量的是在状态 $s$ 下采取动作 $a$ 比平均水平好多少，正是因为这个平均的效应，$A(s,a)$在迭代中方差更小。

现在，还是需要知道函数$Q$和$V$才能进行迭代，虽然二者都可以用神经网络来拟合，但是我们不希望建两个模型，所以我们进一步推导其关系，最终只用$V$来参与迭代。

在一个既定的策略$\pi$下，动作价值函数$Q_\pi$和价值状态函数$V_\pi$是确定的，此时记一局游戏的第$t$步的状态$S_t=s_t$，采取的动作为$A_t=a_t$，得到的收益为$R_t=r_t$，下标$t=1,2,\cdots$。

首先有定理1：
$$
Q_\pi(s_t,a_t) = \mathbb{E}_{\tau\sim\pi} [r_t + \gamma V_\pi(s_{t+1}) | s_t,a_t]
$$
> 该定理可从定义直接推出。
> $$
> \begin{align}
> Q_\pi(s_t,a_t) 
> &= \mathbb{E}_{\tau\sim\pi}[G_t|s_t,a_t] \\
> &= \mathbb{E}_{\tau\sim\pi}[ r_t+\gamma r_{t+1}+\gamma^2t_{t+2}+\cdots|s_t,a_t] \\
> &= \mathbb{E}_{\tau\sim\pi}[ r_t + \gamma G_{t+1}|s_t,a_t]\\
> &= \mathbb{E}_{\tau\sim\pi}[r_t|s_t,a_t] + \gamma\mathbb{E}_{\tau\sim\pi}[G_{t+1}|s_t,a_t]\\
> &= \mathbb{E}_{\tau\sim\pi}[r_t|s_t,a_t] + 
> 	\gamma\mathbb{E}_{s_{t+1}}\Big[ \mathbb{E}_{\tau\sim\pi}[G_{t+1}|s_t,a_t] \Big| s_{t+1} \Big]
> 	\ \text{(全期望公式)}\\
> &= \mathbb{E}_{\tau\sim\pi}[r_t|s_t,a_t] + 
> 	\gamma\mathbb{E}_{s_{t+1}}\Big[ \mathbb{E}_{\tau\sim\pi}[G_{t+1}|s_{t+1}] \Big| s_t,a_t \Big]\\
> &= \mathbb{E}_{\tau\sim\pi}[r_t|s_t,a_t] + 
> 	\gamma\mathbb{E}_{s_{t+1}}\Big[ V_\pi(s_{t+1}) \Big| s_t,a_t \Big] \\
> &= \mathbb{E}_{\tau\sim\pi} [r_t + \gamma V_\pi(s_{t+1}) \Big| s_t,a_t] \\
> \end{align}
> $$
> 这个证明是我根据GPT的回答和搜到的教程拼凑的，权当理解，非严谨证明。

因此有
$$
Q_\pi(s_t,a_t)
≈ r_t + \gamma V_\pi(s_{t+1}) \\
$$
从而
$$
A(s_t,a_t)
= Q_\pi(s_t,a_t)-V_\pi(s_t)
≈ r_t + \gamma V_\pi(s_{t+1}) - V_\pi(s_{t})
$$
所以实际是用下式来作为优势函数：
$$
A(s_t,a_t) ≈ δ_t = r_{t}+γV_\pi(s_{t+1})−V_\pi(s_t)
$$
$\delta_t$被称为TD误差，TD是时间差分（Time Difference），代表下一时刻和当前时刻的价值之差。

现在，我们不再需要拟合$Q_\pi$，只要有一个Critic网络拟合$V_\pi$，再加上本来的Actor网络学习策略$\pi_\theta$，优化这两个网络就够了。Critic网络是在策略$\pi$的前提下工作的，其参数为$w$，所以我们用$V_w$取代$V_\pi$来表示它。

为了能拟合$V_w$，还需要给出这个网络的损失函数。由$V_w$的定义和定理1得出定理2：
$$
\begin{align}
V_w(s_t)
&= \mathbb{E} _{a_t}(Q_\pi(s_t,a_t)|s_t) \\
&= \mathbb{E} _{a_t}(\mathbb{E}_{\tau\sim\pi} [r_t + \gamma V_w(s_{t+1})]|s_t) \\
&= \mathbb{E}_{\tau\sim\pi} [r_t + \gamma V_w(s_{t+1}) | s_t] \\
\end{align}
$$

> 这个证明同样是我根据GPT的回答和搜到的教程拼凑的，权当理解，非严谨证明。

因此可以用$r_t + \gamma V_w(s_{t+1})$作为$V_w$的“真实”值，$V_w(s_{t})$作为预测值，此时误差恰是$\delta_t^2$。

到这一步，我们终于可以给出A2C的计算过程：

---

1. 初始化 Actor 网络 $\pi_\theta(a|s)$ 的参数 $\theta$；Critic 网络 $V_w(s)$ 的参数 $w$；设置学习率 $\alpha_\theta,\alpha_w$，折扣因子 $\gamma\in(0,1)$。

2. 进行一局游戏（采样一个序列$\tau$），其中每一步都根据$\pi_\theta$来采样出行动$a_t$，记录$t=0,1,...,T$的整个序列。

3. 更新 Critic 网络$V_w(s)$：
   - 计算出一系列$δ_t = r_{t}+γV_w(s_{t+1})−V_w(s_t)$（边界$\delta_T=r_T-V_w(s_T)$）
   - 计算损失$L_w=\frac1T\sum_{t=0}^T\delta_t^2$
   - 更新参数$w\leftarrow w-\alpha_w\nabla_wL_w$
4. 更新 Actor 网络$\pi_\theta(a|s)$：
   - 计算损失$L_\theta = -\frac1T \sum_{t=0}^T \log\pi_\theta(a_t|s_t)\cdot\delta_t$
   - 更新参数$\theta \leftarrow \theta- \alpha_\theta\nabla_\theta L_\theta$
5. 重复过程2-4，直到两个网络均收敛



# 参考

A2C的相关定理：https://avandekleut.github.io/a2c/

一些证明是GPT给的。
