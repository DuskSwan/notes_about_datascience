[toc]



# 决策树

## 思路

对于一些被划入$C$个类别的样本（其余特征则都是离散变量），我们希望按照一系列分类标准，将所有$C$个类别分开，这一系列分类标准呈现树的形状，每次判断后都走入一条分支，最终的“叶子结点”就是归属的类别。下图是一个例子：

![](img/决策树.jpeg)

假设每个样本$x_i=(x_{i1},...,x_{ip})$有$p$个特征，每个样本属于$k$个类别中的一种。最开始，全部样本同属于根节点。在每个节点$G$处（不妨设此节点处有$m$个样本），我们希望选择某个特征作为分类依据（比如，这个特征大于20划入一类，小于20划为另一类），将样本按照该特征的值分割为多个子节点，每个叶子都是一个分类结果。

为了衡量选择的优劣，需要考量节点的混乱度：每次分割都可能使得节点更混乱，穷尽所有可能的分类依据，使得混乱度增加最少的那个分类依据，就是好的分类依据。

> 所谓的“混乱”，指的是这样的概念：对于一个节点（样本空间），其中的样本（变量）可能会属于多个类别（样本点），如果所有样本都是同一类，那么任抽一个样本，它的类别归属是唯一的，归类结果确定；如果这些样本大多属于一类，剩下的零散分布在其它类中，那么任抽一个样本，我们可以说它大概率属于最多的那一类，归类结果有倾向；如果抽取的样本等概率地落入多个类别，那么归类的结果最混乱、最难以预测。显见，这三种局面的不确定程度是逐渐增大的。为了衡量结果的不确定程度，我们希望用某个函数$I(G)$来描述节点$G$的不确定性，用$I(G|X_j,f)$来描述选取第$j$特征、依分割标准$f$分割之后，新产生的子节点的不确定性总和，二者的差$I(G|X_j,f)-I(G)$就是“混乱度的增量”。
>
> 这种“不确定”，其含义与混乱、均匀、不纯同质；相对的则是确定、有序、不均、纯粹。我们的目标是将各类别区分开，所以越有序、越不混乱更好，也即混乱度增加越小越好。数学形式的函数会写在“细节”部分。

依次（此处应该可以有多种顺序，比如按照节点产生的顺序，按照树的前序遍历等等）按照上面的方式划分树的节点，直到满足某个标准（比如叶子结点总数够多，树的深度够大，叶子的样本量够小，叶子的纯度够高等等）才停止。现在，全部的数据可以用树状结构来储存，每个叶子节点中样本类别的众数，就是该叶子节点被判为的类别。对于一个新样本，只要判断它应该属于哪个叶子节点，就能判别它的类别。

## 细节与改进

### 1、不确定性的度量函数

假设在节点$G$处，有$m$个样本，分别属于$C$个类别$c_1,c_2,...c_C$。每个样本有$p$个特征，用$X_1,X_2,...,X_p$来表示。对于第$j$个特征，它有$m_j$个可能的取值，对于连续型特征，样本取值通常不会重复，$m_j$通常达到$m$；而对于离散型变量，其类别数通常小于样本数，也即$m_j<m$。将$X_j$可以取的值记为$x_1^{(j)},x_2^{(j)},...,x_{m_j}^{(j)}$。

用$p(c_i|G)$记录属于$G$处属于类$c_i$的样本占$G$ 处总样本数的比例（频率），这实际上是对$x\in c_i$这件事的发生率的估计。显然有$\sum\limits_{i=1}^C p(c_i|G)=1$。

有以下几种方法定义节点划分前后的”不确定性“（或者叫不纯度、混乱度、均匀度）：

（1）信息熵

用
$$
\begin{align}
I(G) = -\sum_{i=1}^{C} p(c_i|G)\log_2(p(c_i|G)) \\
\end{align}
$$
衡量节点$G$分割前的不确定性，称为（信息）熵不纯度。这实际上就是用频率替换概率代入了信息熵的公式。

显见，当节点中的类别均匀分布，信息熵取最大值$\log_2C$；当节点中仅有一个类别，信息熵取最小值$0$。值得注意的是，式中默认了$0\log_20=0$，这实际上是一个极限过程。

假设依据特征$X_j$、按照某种规则$f$将$G$分割成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则它们的总混乱度用加权平均数
$$
I(G|X_j,f)
=\sum_{k=1}^K\frac{m_k}m I(G_k)
$$
来表示。用信息熵的下降值$ I(G)-I(G|X_j,f)$来衡量分割的好坏（可以证明分割一定会使信息熵下降），下降值越大，则分割后越有序，则越好。该下降值称为信息增益（InfoGain)。

实践中，信息增益倾向于将节点分割得很细、每个叶子节点都是单一类的（纯粹的），这会导致过拟合问题。对此的一个改进是，用“分裂信息量”来限制分裂出的节点数。

假设节点$G$分裂成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则定义分裂的信息量为
$$
\text{SplitInfo}=-\sum_{k=1}^K \frac{m_k}{m} \log_2 (\frac{m_k}{m})
$$
可以看出，$K$越大，这个值也会越大。用信息增益率
$$
\text{InfoGainRatio}
=\frac{\text{InfoGain}}{\text{SplitInfo}}
=\frac{ I(G)-\sum\limits_{k=1}^K\frac{m_k}m I(G_k)}{-\sum\limits_{k=1}^K \frac{m_k}{m} \log_2 (\frac{m_k}{m})}
$$
代替信息增益作为衡量分割优劣的依据，就能避免分割出过多的叶子结点。

（2）Gini指数

节点$G$处的Gini指数为
$$
I(G)=\operatorname{Gini}(G)
=\sum_{i=1}^C p(c_i|G) \big(1-p(c_i|G)\big)
=1-\sum_{i=1}^C \left[p(c_i|G)\right]^2
$$
显见，当节点中的类别均匀分布，Gini指数取最大值$1-\frac1C$；当节点中仅有一个类别，Gini指数取最小值$0$。

假设依据特征$X_j$、按照某种规则$f$将$G$分割成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则它们的总混乱度用加权平均数
$$
I(G|X_j,f)
=\operatorname{Gini}_{\text{split}}(G)
=\sum_{k=1}^K\frac{m_k}m \text{Gini}(G_k)
$$
来表示。由于Gini指数是小于$1$的，加权平均数一定会比原来更小，所以变化量是负值。我们于是用Gini指数的下降值$ I(G)-I(G|X_j,f)$来衡量分割的好坏，下降值越大，则分割后越有序，则越好。

（3）误分率

节点$G$处的误分率定义为
$$
I(G)=\text{Error}(G)
=1-\max\limits_{1\leqslant i\leqslant C} \{ p(c_i|G) \}
$$
可见，一个节点中如果某一类占的比重很大，误分率就会比较小；如果各个类比较均匀，误分率就比较大。所以这也是衡量混乱度的指标。

假设依据特征$X_j$、按照某种规则$f$将$G$分割成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则它们的总混乱度用加权平均数
$$
I(G|X_j,f)
=\operatorname{Error}_{\text{split}}(G)
=\sum_{k=1}^K\frac{m_k}m \text{Error}(G_k)
$$
来表示。同理，一个节点被拆分后，混乱度下降越多越好。

（4）方差

前述的度量都针对离散型目标（这是自然的，因为决策树的目的就是分类）。但实际上，即使目标是连续的，我们也可以通过划分连续变量的分布区间来使之离散化，用一个代表性的值（比如均值）来作为这个区间所有样本的目标特征（标签、响应变量）。从这个角度看，决策树也可以用于做回归，此时可以称之为回归树。

针对连续型目标，无法使用前述的度量，所以我们考虑用方差衡量节点分裂前后的混乱度。

具体来说，假设在节点$G$处，有$m$个样本，它们的目标特征分别是$y_1,y_2,...,y_m$（一般来说不会出现重复）。那么节点$G$处的混乱程度就是$(y_1,y_2,...,y_m)$的方差$I(G)=V(y)=\displaystyle\frac{1}{m-1}\sum_{i=1}^m(y_i-\bar y)^2$。

针对连续型目标，应该选择一个特征的一个取值作为阈值，将节点分裂成两个子节点。分裂后的“总方差”是与上同理的加权平均值还是直接加和？暂时不知。我认为两种方式的效果应该是相同的。总之，分裂之后的“总方差”应该小于分裂前大方差，减小值越多，则这个分裂点选择越优。

### 2、划分规则

对一个节点$G$，选定特征$X_j$，如何选择哪些样本进入哪些子节点、总共多少个子节点？划分的具体规则到底是什么呢？

最初提出的决策树算法ID3（Iterative Dichotomiser 3）只能处理离散型特征与离散型标签（目标、响应变量）。选定特征$X_j$（它有$m_j$个可能的取值）之后，样本们的$X_j$取值为$x_1^{(j)},x_2^{(j)},...x_{m_j}^{(j)}$的各自划入一个子节点，共$m_j$个子节点。使用信息熵来度量混乱度。

接下来出现了算法C4.5，它在面对离散型特征时，依然采取如上的“一值一类”的拆分规则；在面对连续型特征时，则从$X_j$可能取到的值中选一个作为分裂阈值，如果选了$x_i^{(j)}$，则将$X_j\leqslant x_i^{(j)}$的样本归入一个节点，其余的$X_j > x_i^{(j)}$的样本放入另一个节点，仅有两个子节点。现在，C4.5既可以处理离散型特征，又可以处理连续型特征了，但是目标依然只能是离散的（分类变量）。此外，C4.5还用信息增益率代替了信息增益来衡量分裂的优劣。

新的方案是CART（Classification And Regression Tree），不论是按照离散型特征还是连续型特征，它都只分裂为两个节点。连续型特征的处理方案与C4.5相同；针对离散型特征，则只分为$X_j= x_i^{(j)}$和$X_j\neq x_i^{(j)}$这两组。此外，CART针对离散型目标使用Gini指数，针对连续型目标使用方差，来度量混乱度。

### 3、优化方法-剪枝

为了避免决策树太过复杂，产生过拟合，我们希望减除一些不必要的分裂。主要有预剪枝和后剪枝两种思路。

预剪枝会设定一个“混乱度下降阈值”，如果某个节点在分裂后的混乱度下降值不超过阈值，则认为这个分裂没必要进行，直接将该节点视为叶子结点即可。

后剪枝通过指标——整体损失函数，来衡量树的优劣。越小的整体损失意味着树越好。先建立完整的树，考虑是否要进行一些剪枝操作。如果剪枝之后的整体损失函数变小，就说明这次剪枝可取，否则不可取。遍历全部的剪枝方案，最终可以让整体损失函数达到（局部）最小。

假设一棵树$T$共有$|T|$个节点，记为$G_1,G_2,...,G_{|T|}$，它们所蕴含的样本数分别是$n_1,n_2,...,n_{|T|}$，选定某种混乱度度量函数$I(G)$，那么这棵树的整体损失函数（也可称复杂性代价）定义为
$$
R_\alpha(T)=\text{Cost}_\alpha(T)
=\sum_{t=1}^{|T|} n_t I(G_t) +\alpha|T|
$$
其中的$\alpha$是复杂度控制参数，$\alpha$越大，越倾向于选择节点数少的树。

## 代码

1、分类树

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier

```python
#签名
class sklearn.tree.DecisionTreeClassifier(*, 
	criterion='gini', 
	splitter='best', 
	max_depth=None, 
	min_samples_split=2, 
	min_samples_leaf=1, 
	min_weight_fraction_leaf=0.0, 
	max_features=None, 
	random_state=None, 
	max_leaf_nodes=None, 
	min_impurity_decrease=0.0, 
	class_weight=None, 
	ccp_alpha=0.0)
#样例
from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
```



超参数：

​	criterion：不纯度度量。默认'gini'即使用Gini指数度量。还可以选择'entropy'即信息熵。

​	max_depth：树的最大深度。如果不指定，则会一直计算直到叶子节点全部纯净、或者内部样本量足够小。

​	min_samples_leaf：叶子节点的最小样本数。如果是整数则直接指定；如果是小数，则视为是占全体样本的比例（也即用`ceil(min_samples_leaf * n_samples)`作为最小样本数）

​	min_impurity_decrease：不纯度下降的最大值。用于在预剪枝中筛选优秀子树。

​	ccp_alpha：整体损失函数的参数$\alpha$。这一参数需要sklearn版本0.22才有。

属性：

方法：

​	fit(X,Y)：训练。其中X应该是记录了特征的二维数组，Y是记录了标签的一维数组。

其他操作：

```python
#绘制分类树
>>> tree.plot_tree(clf) 

#以文本形式输出树
>>> r = export_text(clf, feature_names=df['feature_names'])
>>> print(r)
|--- petal width (cm) <= 0.80
|   |--- class: 0
|--- petal width (cm) >  0.80
|   |--- petal width (cm) <= 1.75
|   |   |--- class: 1
|   |--- petal width (cm) >  1.75
|   |   |--- class: 2
```

2、回归树

签名：

```Python
class sklearn.tree.DecisionTreeClassifier(criterion='mse'
                                          ,splitter="random"
                                          ,max_depth=None
                                          ,min_samples_split=2
                                          ,min_samples_leaf=1
                                          ,min.weight_fracton_leaf=0.0
                                          ,mac_features=None
                                          ,random_state=None
                                          ,max_leaf_nodes=None
                                          ,min_impurity_decrease=0.0
                                          ,min_impurity_split=None
                                          ,presort=False
                                         )

```

超参数：

（大多数都和决策树含义相同）

​	criterion：回归树衡量分枝质量的指标，支持的标准有三种。"mse"使用均方误差；"friedman_mse"使用费尔德曼均方误差，这种指标是用费里德曼对潜在分支中的问题改进后的均方误差；输入 "mae"使用绝对平均误差MAE（mean absolute error）

方法：

​	fit(X,Y)：训练。其中X应该是记录了特征的二维数组，Y是记录了标签的一维数组。

​	score(*X*, *y*)：针对测试集X、和真实的标签y，计算判定系数。

​	

# 随机森林

## 思路

如果只有一棵决策树，难免出现过拟合或者欠拟合的情况，结果的偏差在所难免。但如果有很多棵相互无关的决策树，各自独立地做出判断，投票得出判别结果，就大大降低了出差错的风险。这样由多棵树共同组成的分类模型，就成为森林。而为了让决策树之间互不相关，每棵树只随机抽取一部分样本来建立，于是称为随机森林。在实践中，也可以建立在其他各种分类器上（而不是必须依赖决策树）。

## 代码

1、随机森林分类器

在该随机森林中，使用决策树作为基本分类单元。

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=forest#sklearn.ensemble.RandomForestClassifier

签名：

```python
class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, 
                                              criterion='gini', 
                                              max_depth=None, 
                                              min_samples_split=2, 
                                              min_samples_leaf=1, 
                                              min_weight_fraction_leaf=0.0, 
                                              max_features='auto', 
                                              max_leaf_nodes=None, 
                                              min_impurity_decrease=0.0, 
                                              bootstrap=True, 
                                              oob_score=False, 
                                              n_jobs=None, 
                                              random_state=None, 
                                              verbose=0, 
                                              warm_start=False, 
                                              class_weight=None, 
                                              ccp_alpha=0.0, 
                                              max_samples=None)
```

超参数：

​	n_estimators：森林中决策树的数量。

​	criterion：不纯度度量。可以是Gini指数'gini'或者信息熵'entropy'。

​	max_depth：决策树的最大深度。

​	max_features：建立每棵树所采用的最大特征值数。传入整数直接表明数量；浮点数表示占全部特征的比例；None表示使用全部特征；“auto”等价于“sqrt”表示使用全部特征数的平方根；“log2”表示使用全部特征数的以二为底的对数。

​	min_samples_split：待分裂节点的最小样本数。样本数小于该节点的不会再分裂。

​	min_samples_leaf：决策树的叶子节点的最小样本数。如果是整数则直接指定；如果是小数，则视为是占全体样本的比例（也即用`ceil(min_samples_leaf * n_samples)`作为最小样本数）

​	max_leaf_nodes：决策树的叶子节点的最大数目。用于在后剪枝中控制树的复杂度。

​	min_impurity_decrease：不纯度下降的最大值。用于在预剪枝中筛选优秀子树。

​	bootstrap：是否抽样。如果是，则每次抽样建立决策树；如果否，则每次用全部数据建立决策树。

​	oob_score：仅在需要抽样（bootstrap=True）时生效。是否要使用抽样样本之外的样本评估泛化得分。

​	random_state：在涉及的随机函数中充当随机种子。如果传入一个整型，就拿它当做随机种子。此外也可以传入一个numpy.random.RandomState对象。

​	ccp_alpha：整体损失函数的参数$\alpha$。这一参数需要sklearn版本0.22才有。

​	max_samples：仅在需要抽样（bootstrap=True）时生效。设定每棵树在抽样时的样本数。默认等于全部样本数；传入整数则直接使用；传入小数则视为占全部样本数的比例。这一参数需要sklearn版本0.22才有。



属性：

​	estimators_：由树组成的列表。每棵树是一个DecisionTreeClassifier对象。

​	classes_：目标特征。也即类别组成的数组。

​	oob_score_：该属性仅在oob_score=True时产生。是使用抽样样本之外的样本估计获得的训练数据集的分数。



方法：

​	fit(*X*, *y*)：根据样本训练模型。

​	apply(*X*)：针对数据集X，返回每棵树对每个样本的拟合结果。以(n_samples, n_estimators)形式的数组给出。

​	predict(*X*)：针对数据集X进行预测。

​	predict_proba(*X*)：针对数据集X，返回每个样本属于各个类别的概率。以(n_samples, n_classes)形式的数组给出。

​	score(*X*, *y*)：针对测试集X、和真实的标签y，计算预测的平均正确率。



# GBDT

GBDT（Gradient Boosting Decision Tree，梯度提升决策树）是一种集成学习方法。它的核心思想是使用梯度提升算法（Gradient Boosting），通过迭代地优化损失函数，来逐步改进模型的预测效果。

梯度提升算法的基本思想是逐步构建模型，每一步都在减少模型的损失函数。具体到GBDT，它每次迭代都会添加一个决策树，尝试拟合上一棵（准确地说是一群）决策树的损失函数负梯度，以降低总体损失函数。这个过程就像是在进行梯度下降，不过这里的"梯度"是通过构建新的树来近似的。

> 为什么不直接拟合预测值和真实值的误差，而是用损失函数的负梯度？事实上，损失函数的梯度才是误差的“通用”衡量方法，绝对误差可以看作一个特殊的梯度。损失函数梯度实际上是误差的一种推广。如果损失函数是凸的，且只有在预测值等于真实值时梯度才会为0，那么向着负梯度方向前进必然收敛到最优解。

## 过程

假设有$n$个样本，记为 $\{(x_i, y_i)\}, i = 1, \dots, n$，GBDT的计算过程可以详细分解为以下几个步骤：

---

1.**初始化模型**

GBDT从一个简单的常数模型开始，我们希望模型得到的结果是一个最能描述所有样本的常数：

$$
F_0(x) = \arg \min_c \sum_{i=1}^n L(y_i, c)
$$

在回归任务中，通常将 $F_0(x)$ 初始化为目标值 $y$ 的平均值（分类任务是大概是众数吧，我猜的）：
$$
F_0(x) = \frac{1}{n} \sum_{i=1}^n y_i
$$

2.**迭代训练决策树**

GBDT逐步构建模型，每一轮（即每一棵新树）都拟合当前模型的损失函数负梯度（对均方损失来说恰是残差），第 $m$ 轮迭代为：

（1）计算负梯度（残差）： 

对每个样本，计算当前模型预测值与真实值之间的误差（损失函数的负梯度），得到每个样本的残差：
$$
r_i^m = -\left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x) = F_{m-1}(x)}
$$

> 为什么是对$F$求导？一般的损失函数都是对参数$\theta$求导吧？这是因为求导对象是我们要解的对象。一般的场景里我们要找到的是最优的参数$\theta$，但现在我们要求的就是$F$本身，这个$F$并不像一般的场景中是由$\theta$决定的，而是用其他手段（建立决策树）得到的。

对于均方误差（MSE）损失函数 $L(y, F(x)) = \frac{1}{2} (y - F(x))^2$，负梯度等于实际值和预测值之差：
$$
r_i^m = y_i - F_{m-1}(x_i)
$$
对于其他损失函数，如对数损失等，残差计算会有所不同，但本质上都是通过负梯度来表示残差。

（2）拟合残差（训练新的决策树）：

使用上一轮计算的残差 $r_i^m$ 作为目标值，拟合一个新的决策树 $h_m(x)$，此决策树的任务是尽可能精确地预测这些残差$h_m(x) \approx r_i^m$。

> 再次强调，每个决策树的拟合对象是”负梯度“而不是”误差“，每个决策树的结果扮演的是”损失函数的自变量“，多个决策树的结果相加就好像自变量一步步移动到了损失函数的全局最小化值。

（3）更新模型： 

将当前模型更新为之前模型和新树的组合，更新公式如下：
$$
F_m(x) = F_{m-1}(x) + \alpha h_m(x)
$$
其中，$\alpha$ 是学习率（通常设置为较小值，如 0.1 或更小），控制每棵树对模型的贡献，防止模型过拟合。

> 这里也可以看出，$h_m(x)$拟合的对象是损失函数负梯度而不是简单的预测绝对误差。如果是后者，应该直接加进去就好了，而拟合负梯度时才需要乘学习率，$\alpha h_m$加进$F_{m-1}$可以看作是自变量$F_{m-1}(x)$向着负梯度方向$f_m(x)$移动了一步。

（4）重复迭代： 

重复上述步骤，继续构建决策树 $h_1(x), h_2(x), \dots, h_M(x)$，直到达到设定的树的数量 $M$ 或模型误差收敛。

3.**GBDT的最终模型**

经过 $M$ 轮迭代后，GBDT模型的最终输出为所有基学习器的加权和：

$$
F(x) = F_M(x) = F_0(x) + \sum_{m=1}^M \alpha h_m(x)
$$

这个模型将每一棵树的预测结果进行累加，从而形成一个强大的预测模型。



# XGBoost

XGBoost可以看作是GBDT的改进版本，它对GBDT算法进行了许多优化。XGBoost的全称是“eXtreme Gradient Boosting”，其主要优化包括支持分布式计算、使用贪心算法进行节点分裂、添加L1和L2正则化、对并行计算的支持等。这些优化显著提升了模型的速度、准确性和稳定性。

以下只解释XGBoost相对GBDT改进的地方，不再赘述全流程。

## 改进

### 损失函数正则化

XGBoost的损失函数为
$$
L(y,\hat y) = \sum_{i=1}^n l(y_i,\hat y_i) + \sum_{k=1}^K \Omega(f_k)
$$
其中$y_i$和$\hat y_i$分别是每个样本的真实值和预测值，$f_k$是依次建立的一系列决策树。$\Omega(f)$就是正则化项，它实际上描述了一棵决策树$f$的复杂程度。

假设决策树$f$总共有$T$片叶子，每片叶子有一个权重值 $w_j \in R$，那么正则化项可以表示为
$$
\Omega(f)=\gamma T+\lambda \sum_{j=1}^T w_j^2
$$
其中$\gamma$和$\lambda$是系数。叶子节点的权重是什么意思，如何计算呢？待补充




# LightGBM

LightGBM是由微软开发的一种基于GBDT的优化模型，主要用于处理大规模数据。它使用基于直方图的分割方式，分割数据更加快速、内存占用更低，尤其适用于大数据集和高维稀疏数据。LightGBM引入了基于叶子（leafwise）的生长策略，与XGBoost的基于层的生长策略不同，使其能够捕获更复杂的模式。



# Isolation Forest

Isolation Forest（孤立森林）是一种基于树结构的无监督异常检测算法，由周志华等人于2008年提出。它的核心思想是：异常点是少数且与正常点显著不同，因此更容易被“孤立”出来。

假设有一堆数据点，其中大部分是正常的，只有少数是异常的。其核心思想是，要将正常点从群体中“孤立”出来，需要进行多次随机的切分，才能把它们分到单独的区域。而异常点散落在数据空间中，远离正常点。要将它们“孤立”出来，通常只需要进行少量几次随机切分，就可以把它们分到一个单独的区域。

利用这个特性，Isolation Forest构建了一系列随机的iTree（孤立树），每棵树都尝试将数据点孤立起来。

## 过程

假设有$n$个样本，训练/构建孤立森林的过程如下：

1. **随机抽样**：从训练数据中随机抽取一个固定大小的子样本（例如256个点），作为一棵iTree的训练数据。这样做的目的是减少计算量，并引入随机性。
2. **随机分裂**：对于每个iTree，从根节点开始，递归地进行分裂：
   - **随机选择一个特征**：从所有特征中随机选择一个特征。
   - **随机选择一个分裂点**：在所选特征的最大值和最小值之间随机选择一个分裂点。
   - **划分数据**：根据这个分裂点，将数据划分为两个子集，分别进入左右子节点。
3. **停止条件**：分裂过程会一直进行，直到满足以下任一条件：
   - 节点中只有一个数据点。
   - 达到预设的最大树深度（为了避免过拟合和控制计算量）。
   - 节点中所有样本的所有特征都相同，无法再进行分裂。
4. **重复构建**：重复步骤1-3多次，构建多棵独立的iTree，这些iTree共同组成了Isolation Forest。

构建好模型之后，对于一个新样本，判断其是否异常的过程如下：

1. **遍历每棵树**：对于一个待检测的数据点，将其从每棵iTree的根节点开始向下遍历，直到它到达叶子节点。

2. **计算路径长度**：记录该数据点在每棵iTree中从根节点到叶子节点的路径长度（即分裂次数）。路径长度越短，说明该点越容易被孤立。

3. **计算平均路径长度**：将该数据点在所有iTree中的路径长度进行平均。

4. **计算异常分数**：根据平均路径长度，计算一个异常分数（Anomaly Score）。异常分数的计算公式为
   $$
   s(x,n)=2^{-\frac{\bar h(x)}{c(n)}}
   $$
   其中$ x $是待检测的数据点，$n$是训练样本的数量，$\bar h(x)$是数据点$ x $在所有iTree中路径长度的平均值。

   $c(n) $是给定样本量$ n $的iTree平均路径长度的标准化常数，用于对路径长度进行归一化。它的公式通常为$c(n)=2H(n−1)−\frac{2(n−1)}{n}$，其中$ H(k)=\sum_{i=1}^k\frac1i$是调和数，可以用$ \ln(k)+γ $来近似（$γ$ 是欧拉-马斯刻若尼常数）。

5. **异常分数的解读：**

   平均长度$\bar h(x)$和标准化常数$c(n)$都是正数，所以幂是负数，异常分数$s$为0-1之间的数。若平均长度小，指数接近0，异常分数接近1，数据点很可能是异常值；平均长度大，异常分数远小于0.5，数据点很可能是正常值；平均长度与标准长度相同，异常分数接近0.5，则难以区分，需要进一步分析。

> 关于其中为什么异常分数的公式长这个样子，$c(n)$其实是给定样本量 $n$ 的二叉查找树 (Binary Search Tree, BST) 的平均路径长度的理论值，归一化是为了排除样本量$n$的影响。选2为底数而不是自然对数e，则是为了让异常阈值$2^{-1}=0.5$比较简单。



# 参考

解释XGBoost：https://blog.csdn.net/v_JULY_v/article/details/81410574