[toc]



# 思路

## 基本概念

强化学习 (Reinforcement learning, RL) 讨论的问题是，一个智能体 (agent) 如何在复杂的环境(environment) 中行动，来达成其目的。每一轮行动中，智能体要感知环境的状态 (state) 或者说获取环境的观察值 (observation)，采取一个动作 (action)，然后获得相应奖励 (reward)。一个好的策略能够使得智能体最终获得较高的奖励，学习这个好策略的过程就是强化学习。

用数学语言定义强化学习的过程：

我们的智能体agent面对着$X$这一状态空间，每个$x\in X$是一个状态；可采取的全部行为构成则空间$A$，每个$a\in A$是一个可采取的行为。在状态$x$处采取行为$a$，可能会导致多种新状态出现，假设出现状态$y(y\in X)$的概率为$p$，那么函数$P: X\times A\times X\mapsto \mathbb{R},P(x,a,y)=p$就描述了所有可能的状态转移概率。为了评价一次转移的好坏，定义奖励函数$R:X\times A\times X\mapsto \mathbb{R}$来描述”状态$x$下采取行为$a$转移到状态$y$“的得分。

上述的$(X,A,P,R)$确定下来后，agent所面对的环境就完全确定了，其任何行为都会有准确的评分。这样的环境$E=(X,A,P,R)$下，我们希望机器给出一个策略 (policy) $\pi$。策略有两种形式，确定性策略是函数$\pi:X\mapsto A,\pi(x)=a$，表示遇到状态$x$就采用行为$a$；随机性策略则是函数$\pi:X\times A\mapsto \mathbb{R},\pi(x,a)=p$，表示在状态$x$下有$p$的概率选择行为$a$，这里还要求有$\sum_a\pi(x,a)=1$。

为了评价一个策略的好坏，考虑长期执行策略后的累计奖励。常用的价值函数有两种，一是状态价值函数$V_\pi(s)$，表示从某个状态s开始，遵循策略$\pi$所能获得的预期累积奖励；二是动作价值函数$Q_\pi(s,a)$，表示在状态$s$下采取动作$a$，然后遵循某个策略$\pi$所能获得的预期累积奖励。

~~严格地定义一下价值函数，假设按照策略$\pi$，第$t$步（执行第$t$个动作）是在状态$x$下以概率$p_i$执行动作$a_i$（$i=1,2,...$表示有多种选择），该步获得的期望奖励就是$r_t=\sum_i p_iR(x,a_i,y_i)$，从状态$s$开始共执行了$T$步，那么期望$E(\frac1T\sum_{t=1}^Tr_t)$便可以作为一个评价指标。当然，还可以定义更多指标。~~

求解一个使得价值函数最大的策略$\pi$，就是强化学习的最终目标。

## 分类

可以按照多种角度来对强化学习方法做分类。

 基于模型的 (Model-based) vs. 无模型 (Model-free)：如果环境$E=(X,A,P,R)$能显式写出，或者能用模型近似出，那么这就是一个建立随机过程上的最优化问题，可以求解析解，这就是基于模型的强化学习。反之如果智能体无需显式地构建环境模型，直接从与环境的交互中学习最优策略，就是无模型的。

基于策略 (Policy-based) vs. 基于价值 (Value-based) vs. Actor-Critic：基于策略指的是直接学习出策略，每次给出下一步动作的概率；基于价值则是学习一个价值函数（例如$Q$函数），然后根据这个价值函数来选择动作（选择使$Q$值最大的动作）；Actor-Critic结合了二者，Actor (行动者) 负责学习策略并选择动作，Critic (评论者) 负责学习价值函数并评估Actor选择的动作，同时Critic的评估结果用于指导Actor改进策略。

在线策略  (On-Policy) vs. 离线策略 (Off-Policy)：在线策略指agent一边应用策略交互，一边学习（更新）这个策略；离线策略则可以使用与要学习的策略相异的策略，”使用的策略“和”学习的策略“可以有关也可以无关，甚至可以先交互一通，事后再根据采集的数据学习。

# 经典算法

## Q-Learning

Q-Learning是无模型的、基于价值的、离线的策略。其思路是建立一个Q值表，这个表描述了一个离散函数$Q:(X,A)\mapsto \mathbb{R}$，也即对每个状态$s\in X$，$Q(s,a)$描述了此时采取动作$a$的收益。有了这个表，每次行动时只要选取使得Q值最大的动作即可。

建立这个表的方式是迭代。先让Q值表随机初始化，比如初始全部为$0$，令agent以任何策略行动一番，收集数据，用收集到的数据更新Q值表，最终即可收敛到一个真实有效的Q值表。

在描述Q值表的迭代过程之前，还要考虑一个问题，为什么在$s$处不直接采用使得单步收益最大的动作？这显然是贪心策略，而贪心容易陷入局部最优解，错过全局最优解。因此建立Q值表时不仅要考虑当前一步的收益，还要考虑这一步带来的未来收益，需要一个参数控制“对未来收益的关注程度”，这个参数就记为$\gamma$，称折扣因子，取值范围在 $[0,1]$ 之间。它衡量了未来奖励的重要性。$\gamma$ 接近 $0$ 意味着智能体更关注即时奖励；$\gamma$ 接近 $1$ 意味着智能体更关注长期奖励。

Q值表的迭代公式如下：
$$
Q(s_t,a_t) = Q(s_t,a_t)+α[r_{t+1}+γ\max_{a'}Q(s_{t+1},a')−Q(s_t,a_t)]
$$
$t$表示第$t$步更新，$\alpha$是学习率，这都是迭代公式中的常见部分。这一公式表示每次将$(s_t,a_t)$处的收益值$Q(s_t,a_t)$向着$r_{t+1}+γ\max_{a'}Q(s_{t+1},a')$方向移动，后者代表更加”正确“的收益值。其中$r_{t+1}$是在状态 $s_t$ 下执行动作 $a_t$ 后，单步转移的奖励；而$\max_{a'}Q(s_{t+1},a')$表示下一状态 $s_{t+1}$ 的最大可能Q值，二者相加正衡量了当前$(s_t,a_t)$处的收益值。

> 顺带一提，$r_{t+1}+γ\max_{a'}Q(s_{t+1},a')−Q(s_t,a_t)$这部分被称为时序差分误差。不知道为什么要给它起个名字。

前面我们说的是”令agent以任何策略行动一番“，但实际上这个策略也不能完全随机，不然Q值表将无法收敛，或是收敛需要的时间极长。现实中采用的是$\epsilon$-贪婪 (greedy) 策略。

在行动时，用概率值$\epsilon$表示探索的概率，每一步中，以$\epsilon_t$的概率执行探索——随机选择一个动作，以$1-\epsilon_t$的概率按照Q值表执行最佳动作。$\epsilon_t$初始是$1$，也即在Q值表未建立起来时随机选动作，而后随着$t$逐渐减小到$0$。

一个应用Q-learning的典型例子是迷宫找路。

对于状态空间或动作空间非常大的问题，Q表会变得极其巨大，无法存储和计算。这是Q-learning面临的最大挑战。另外Q表要求离散的状态和动作，无法直接处理连续值。

## SARSA

SARSA 这个名字本身就概括了它的核心更新过程：它需要当前状态 **S**tate、当前动作 **A**ction、获得的 **R**eward、下一个状态 **S**tate'、以及下一个动作 **A**ction'。

SARSA 和 Q-learning 非常相似，它们都是免模型（model-free）、基于价值（value-based）的时序差分（Temporal Difference, TD）控制算法。然而，它们之间有一个关键的区别，这个区别决定了它们是“在线策略”还是“离线策略”算法。

Q-learning执行的是$\epsilon$-贪婪 (greedy) 策略，学习目标则是Q值表（决定的最佳行动策略），二者并不完全相同。SARSA学习的则是在所遵循的特定策略（通常就是 $ϵ$-贪婪策略）下，每个（状态，动作）对的价值。

SARSA的迭代公式如下：
$$
Q(s_t,a_t) = Q(s_t,a_t)+α[r_{t+1}+γQ(s_{t+1},a_{t+1})−Q(s_t,a_t)]
$$
与 Q-learning 的区别仅仅在于， Q-learning 使用 $\max_{a'}Q(s_{t+1},a')$（下一步最优动作的奖励值），而 SARSA 使用 $Q(s_{t+1},a_{t+1})$（下一步实际动作的奖励值）。

这个简单的差异带来的影响是，SARSA会更倾向于“安全”的路径，因为它会考虑探索带来的负面奖励。而Q-learning可以学到最短路径，即使这条路很危险。不妨想象一个网格世界，智能体从起点开始，目标是到达终点。在终点和起点之间有一条“悬崖”，掉入悬崖会受到巨大的负奖励（例如 −100），并回到起点。这时Q-learning会找到到达终点的最短路径，即使这意味着它需要沿着悬崖的边缘行走；而SARSA会倾向于选择一条更远离悬崖的“安全”路径，即使这条路径可能更长。

与 Q-learning 相同，SARSA不适用于状态空间或动作空间非常大的问题，因为需要一个Q表；并且同样需要离散的状态和动作。在某些问题上，SARSA 可能需要更多的时间来收敛，”在线“的性质使得收敛不太容易。

## 深度Q网络（Deep Q-Networks, DQN）

Q-learning无法解决连续型状态或者说状态空间非常大的局面，DQN 的出现正是为了解决 Q-learning 的这个核心局限性，其核心思想是使用一个深度神经网络来近似函数 $Q(s,a)$。

对于参数为$\theta$的网络，自然地，我们考虑用神经网络的迭代公式
$$
\theta_{t+1} = \theta_t + \alpha\nabla_\theta(r_{t+1}+γ\max_{a'}Q(s_{t+1},a')−Q(s_t,a_t))
$$
来更新，但这样存在问题，一是连续的时间步中通常会经历高度相关的状态序列（例如，在游戏中，相邻帧的图像非常相似），差分值极小，难以收敛；二是差分值中包含了当前的 Q 网络本身，这意味着差分值会随着 Q 网络的更新而不断变化，导致训练目标不稳定。

为了解决这些稳定性问题，DQN 引入了两个关键的技术，经验回放和目标网络固定。

经验回放 (Experience Replay) 指的是，智能体与环境交互的经验（也即$(s_t,a_t,r_{t+1},s_{t+1})$，将其作为一个样本）不再立即用于训练。相反，这些经验被存储在一个名为经验回放缓冲区（Replay Buffer）的大型数据库中。在训练时，不再按顺序使用最新的经验，而是从回放缓冲区中随机抽取一个批次（mini-batch）的经验来更新 Q 网络。

目标网络固定指的是，使用两个相同的神经网络。一个称为当前 Q 网络 (Current Q-network) 或在线网络 (Online Network)，用于计算当前的 Q 值 $Q(s_t,a_t)$ 和选择动作。另一个称为目标 Q 网络 (Target Q-network)，用于计算 Q-learning 更新公式中的目标值 $\max_{a'}Q(s_{t+1},a')$​。当前 Q 网络在每个训练步都会更新其权重。目标 Q 网络的权重则定期地、缓慢地（例如，每隔 C 个训练步，将当前 Q 网络的权重复制到目标 Q 网络）进行更新。

由于计算差分值用的是暂时不变的目标 Q 网络，所以差分值不会一直很小，当前Q网络也不会时刻影响差分值。这就解决了前述的问题。

训练时，每一轮循环执行的操作包括：

- 以$\epsilon$的概率随机行动，以$1-\epsilon$的概率选择当前Q网络认为的最佳动作作为$a_t$；
- 根据动作$a_t$计算单步奖励$r_{t+1}$，和新状态$s_{t+1}$；
- 将经验元组（样本） $(s_t,a_t,r_{t+1},s_{t+1})$ 存储到经验回放缓冲区 D 中；
- 训练当前 Q 网络（每隔$n$步）：
  - 从经验回放缓冲区 D 中随机抽取一个批次的样本 $(s_j,a_j,r_j,s_j')$，计算Q值误差$y_j=r_j+\max_{a'}Q_t(s_j',a')-Q_c(s_j,a_j)$（注意这里的$Q_t$表示目标Q网络，$Q_c$表示当前Q网络）；
  - 用误差值来更新当前Q网络的参数，比如用MSE做损失函数，则损失函数就是$L=\frac1b\sum y_j^2$，其中$b$是批次大小；
- 更新目标Q网络（每隔$C$步）：将目标Q网络的参数更新为当前Q网络的。

## 策略梯度（Policy Gradients）

## Actor-Critic 

