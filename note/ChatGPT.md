[toc]

（本文内容在ChatGPT的帮助下完成）

ChatGPT是由OpenAI开发的基于GPT（Generative Pre-trained  Transformer）架构的语言处理模型的一个特定实现，它专门被优化和调整用于生成连贯和有意义的对话文本。它利用了GPT这样的模型作为基础，通过在大量对话数据上进行细致的微调，使其更适合在聊天机器人、对话系统等应用中使用。所谓GPT，是一种基于Transformer的自回归语言模型，即模型能够迭代地根据已经生成的单词来逐个预测后面的单词。

对于对话任务，自回归语言模型是这样的：我们希望给该模型输入一个句子（作为一个矩阵），然后它会根据输入预测一个单词A，再根据输入和单词A预测单词B，再根据输入和单词A、B预测单词C……知道预测出终止符为止。结果序列A,B,C……可以看作是对输入的回应，这就完成了对话。

# GPT1

## 结构

GPT的基础是Transformer的解码器，它直接接受输入然后进行预测。其结构如下：

![image-20240415202052762](img/image-20240415202052762.png)

它堆叠了12个Transformer解码器来提升性能，每一个单独部分的详情可参见[Transformer](Attention&Transformer.md)部分的笔记。其中的自注意力层是带掩码的，掩码可以确保模型在生成文本时，只能关注当前词及之前的词，而不能“偷看”后面的词，这符合语言生成的从左到右的顺序。

解码器的最终输出同样可以接不同的任务头来完成不同的任务，比如在对话任务中，使用的是一个由层归一化、线性层、softmax函数组成的预测器（回归器）。

## 训练

### 编码/词嵌入

上面给出的结构中，模型接受的输入是嵌入向量。事实上GPT模型处理文本数据的第一步是将原始文本转化成嵌入向量，这一步称为词嵌入 (Token Embeddings)。转化过程一般是“文本 → 子词 → ID序列 → 词嵌入向量序列 → 加位置编码”。

子词：将句子分解成子词（subword）的过程叫分词。GPT-1 用名为BPE的分词器，把一句话拆成若干个子词单位，这个过程会在下一节里详细说明。

ID序列：每个子词都有一个对应的词表索引，也就是整数 ID。句子拆解为一系列子词后，根据词表转化成一个 ID 序列，比如 `[154, 27, 999, …]`。

词嵌入向量序列：将这些整数 ID 送入一个查表（embedding layer），每个ID（代表一个词）对应一个嵌入向量，得到一个形状为 `(序列长度, 嵌入维度)` 的浮点向量序列。

位置编码：因为 GPT-1 的 Transformer decoder 自身没有捕捉位置信息的机制，需要给每个子词位置加上一个可学习的“位置嵌入”（position embedding），然后把它和词嵌入按元素相加。这一步和Transformer的位置编码相同。

### 基于字节对编码BPE

在GPT系列模型的训练中，使用一种称为基于字节对编码 (Byte Pair Encoding, BPE) 的算法进行分词，这种算法在Transformer中就开始使用。其核心思路是将常见的、连续出现的字符序列合并为一个单元，这样做可以有效减少整个文本中的字符总数，同时也可以保持文本的易处理性和可读性。

在解码时，BPE分词后的文本可以完全还原，因此被称为可逆分词方法。

以下是BPE算法的详细过程：

1. 准备阶段

   - 收集文本数据：首先需要有一个大型的文本数据集，这将作为词汇分割的基础。通常在每个单词末尾添加后缀 `</w>`，这有利于在分词结束后标明子词是否为后缀。

     > 举例来说：`st` 不加 `</w>` 可以出现在词首，如 `st ar`；加了 `</w>` 表明该子词位于词尾，如 `we st</w>`，二者意义并不相同。

   - 确定词汇表大小：设定一个目标词汇表的大小，这个大小直接影响到算法的细致程度和最终模型的参数数量。


2. 初始化词汇表

   - 分割文本：将所有文本分割成字符级别，包括空格和特殊符号。现在分词结果是由多个单字符字符串组成的。

   - 统计频率：计算每个字符（串）在整个数据集中出现的频率并据此排序。


3. 循环合并

   - 合并：算法会进行多轮合并操作，每一轮选择出现次数最多的相邻字符串对进行合并。
     - 在分词结果中，选择最频繁的相邻字符串对合并为新的单元，例如`('e', 's')`出现最多，则用`'es'`取代`'e','s'`。
     - 根据更新后的语料库更新词汇表。比如合并`'es'`后还剩下了单独了`'e'`，那么词汇表中依然有`'e'`。然后更新每个字符串的频率和排序。

   - 迭代：这一过程迭代进行，直到词汇表达到预定的大小或者所有字符串对的出现频率都是1。


4. 使用词汇表
   - 编码新文本：使用最终的词汇表来将任何文本编码为一系列词汇单元。文本中的词汇通过查找最长的可能单元在词汇表中进行匹配，以此方式编码整个文本。


### 无监督预训练

GPT的训练方式被称为生成式预训练。生成的目标是根据上文$C=\{x_1,...,x_{t-1}\}$，预测出下一个单词$x_t$，本质上是需要学习出分布$P(x_t|C)$。假如用户给定了上下文$C$，设定$x_0=$`<sos>`为起始符，那么只要依次预测$x_1=P(x|C,x_0), x_2=P(x|Cx_0,x_1),\cdots$就可以得到回答$x=\{x_1,\cdots,x_n,\cdots\}$。

要实现这个效果，就要构建一系列合理的“句子”作为样本。GPT-1的预训练使用BookCorpus数据集，这是一个包含超过7000本未出版的免费书籍的语料库。BookCorpus 里的所有文本首先被当作一条条“文档”来处理（每本书、一篇文章视为一个文档），但在文档内部，并不会按“段落”做截断，整个文档会被看成一个连续的子词流（token stream）。在子词流中，按滑动（或随机）方式，每次取 512 个子词 ID 作为一个输入序列，再转化成词嵌入向量并加上位置编码，就得到了一个样本。每个样本是形状为 `(序列长度=512, 嵌入维度=768)` 的矩阵。

对于长度为 512 的序列，前 511 个子词是输入，后 511 个是要预测的目标。模型在每个位置都要做一次“下一个子词”的预测，一共 511 次：

- 第 1 次：用输入 `[x₁]` 来预测 `x₂`
- 第 2 次：用输入 `[x₁, x₂]` 来预测 `x₃`
- …
- 第 511 次：用输入 `[x₁, x₂, …, x₅₁₁]` 来预测 `x₅₁₂`

虽然从概念上是 511 次逐步预测，但在训练时会把整个长度为 512 的输入一次性送进模型，输出一个 `(512, V)` 大小的 logits 张量（V 是词表大小）。然后通过把它右移一位与标签对齐、再计算交叉熵，就等价于同时算出了所有 511 次预测的损失。这就实现了对模型的训练。

为保证位置 t 的预测只能看到位置 < t 的输入，Transformer 的自注意力里会加一个上三角掩码（causal mask），屏蔽掉所有“未来”位置的信息。

当然，GPT也可以在不同下游任务上进行微调。GPT-1 在多个任务上做了微调，包括文本分类自然语言推断等，当时在多数基准上超越了同规模的有监督模型，证明了生成式预训练所学的语言表征能够迁移到多种下游任务上。

## 与BERT比较

GPT（如OpenAI的GPT系列）和BERT（由Google开发）都是现代自然语言处理领域的重要模型，它们都使用了深度学习和注意力机制来处理文本数据。尽管它们在表面上看似类似——都可以通过训练来生成针对输入文本的表示向量，然后使用这些向量完成各种下游任务（如对话生成、文本分类、相似度计算等），但是它们在设计哲学、预训练任务、适用场景等方面存在显著差异。

BERT的核心特点是其双向性，即模型在处理每个输入词时会考虑到整个句子的上下文（both left and right context）。这是通过预训练任务"Masked Language Model"（MLM）实现的，即在训练过程中随机遮挡输入句子中的某些词，然后让模型预测这些被遮挡的词。因为BERT能够捕捉到词在整个句子中的双向关系，它在需要深层次语义理解的任务上表现较好，如问答系统、文本分类、命名实体识别等。

与BERT不同，GPT是单向的，即它只考虑左侧的上下文（或右侧，取决于具体实现）。这意味着在生成某个词时，它只能看到之前的词。GPT通过一个不同的预训练任务"Autoregressive Language Model"（ALM）进行训练，模型需要预测给定文本之后的下一个词。GPT在文本生成任务上表现出色，如聊天机器人、内容生成等，因为它的结构天然适合于生成流畅的、连贯的文本。

# GPT2

## 结构

GPT2在结构上并没有很大的改进，相比GPT1的主要变化是在解码器中将Layer Normalization移到了注意力和前馈神经网络之前，在最终的自注意力块之后又增加了额外的层归一化。（按说这里应该放个图( ╯□╰ )但没找到合适的。）

同时，OpenAI尝试了将编码器堆叠不同次数，给出了不同规模的GPT2。

好吧，结构上的改进实在乏善可陈，GPT2最显著的不同在于解决问题的思路改变，以及由此带来的训练方式改变。GPT2做到了在零样本（Zero-shot）情况下执行下游任务，而无需任何参数或者架构的修改。

为了支持多个零样本任务，GPT-2需要在预训练阶段学习尽可能丰富的数据，因此OpenAI自建了高质量的WebText数据集，只保留了人工过滤过的网页，最终包含4500万个链接。

## 训练技巧

### 正交初始化

网络参数的正交初始化（Orthogonal  Initialization）是一种在初始化神经网络权重时使用的技术，其核心思想是通过正交矩阵来初始化权重矩阵，从而帮助模型在训练初期有更好的收敛性。这种方法尤其适用于深层网络，可以有效减少训练过程中的梯度消失或爆炸问题。

### BBPE

在GPT-2模型中，采用的是一种改进版本的Byte Pair Encoding（BPE）算法，通常称之为Byte-Level BPE（BBPE）。这种方法是对原始BPE的一种优化，使得它更适合处理多种语言，特别是那些使用非拉丁字母脚本的语言。

传统的BPE算法通常在字符级别进行操作，这意味着它会将文本分割成字符（如拉丁字母、汉字等）进行分析和合并。而BBPE则在字节级别上进行操作，处理的基本单元是字节（Byte）。这样做有以下几个优势：

- 语言无关性：字节是所有文本文件存储和传输的基本单位，不依赖于任何特定的语言或字符集。因此，BBPE算法具有很强的语言通用性，可以轻松处理包括中文、阿拉伯文、希伯来文等各种使用非拉丁字母的语言。
- 一致的处理方式：由于所有的文本都被视为字节序列，这消除了处理不同字符系统时的不一致性，简化了算法的实现。

在流程上，BBPE与BPE基本相同，只是在分割文本时将所有文本内容转换成字节序列。在这一步，文本中的每个字符（无论它是拉丁字母、汉字还是其他）都被转换成对应的字节表示（通常是UTF-8编码）。后续以字节为最小单位来进行合并。

## 无监督多任务学习

GPT1的应用过程可以描述为，针对不同任务拼接不同的下游功能头，微调训练功能头后靠功能头完成特定的任务。而GPT2希望取消微调，直接用预训练好的模型完成多种任务。它的输入是一个词序列，输出也是一个词序列。

要实现这一希望，需要将任务（问题）和文本一起输入给模型，基本的想法是通过特定的输入格式来“引导”模型理解并执行给定的任务。对于每个任务，定义一个或多个自然语言的提示，这些提示明确指示模型需要执行的任务。输入序列由提示、分隔符和目标文本组成。常用的分隔符包括特殊符号如“`[SEP]`”或简单的符号如冒号`:`。

例如可以输入`情感分析: 这是一个极好的电影。 <回答> 正面`

在这个例子中，`<回答>`是一个特殊的标记，用于指示模型此处应该输出其对前面文本的情感分析结果。这种格式帮助模型学习到在接到“情感分析:”的命令后，应该怎样处理随后的文本，并在看到`<回答>`标记后输出一个情感类别。

GPT2将自回归式地对每一个样本（句子）依次预测其每个词。最开始，输入到模型的是一个起始标记（在实际操作中，通常会预先定义一个或多个固定的任务描述作为训练样本的一部分），在这里是“情感分析:”；接下来，基于已知的“情感分析:”，模型将预测序列中的下一个词，即“这”；这个过程持续进行，每次生成一个新词，直到整个输入序列预测完成。

> 在自回归预测过程中，会把问题本身也预测出来。预测问题（或任务提示）本身在一些情况下看似没有直接的实用价值，因为这部分文本通常是已知的。然而，这样做有几个潜在的好处和原因：
>
> 1. **学习任务上下文**：
>    - 通过预测整个输入序列（包括任务提示），模型不仅学习如何生成文本，还学习了文本与特定任务之间的关联。这有助于模型在处理真实任务时，更好地理解任务需求和上下文。
> 2. **增强模型的泛化能力**：
>    - 在训练过程中，如果模型能够自行生成任务描述，这表明它已经学会了任务的内部逻辑和相关语言模式，这可以增强其泛化能力，使其在面对未见过的类似任务时表现更好。
> 3. **统一的训练过程**：
>    - 让模型预测整个序列，包括任务提示和答案，可以简化训练流程。这样的统一性使得训练过程更加标准化，便于管理和优化。
>
> 综上所述，虽然预测任务提示本身在某些情况下可能看起来不是直接必需的，但它对于模型的学习过程和最终的功能实现都是有益的，特别是在提升模型对任务的理解和执行能力方面。

# GPT3

## 结构

GPT3的结构与GPT2相似，不同之处在于GPT3堆叠的编码器不相同了，而是一种名为稀疏注意力（Sparse Attention）的新技术。

（网络结构图还是没找到）

GPT-3提供了多个版本，参数从2.5亿到1750亿不等。在最大的模型（如1750亿参数模型）中，OpenAI实验了多种类型的稀疏自注意力机制，这些机制在不同层中有不同的应用：

1. **局部窗口稀疏注意力** - 这种类型的注意力可能被用在模型的初级层（接近输入的层），使模型可以聚焦于输入中的局部特征，类似于我们在图像处理中看到的卷积操作。
2. **全局稀疏注意力** - 在更高层次上，模型可能使用全局稀疏注意力来捕获文本中的长距离依赖。这种注意力模式允许模型在整个文本输入中选择性地关注信息，这对于理解复杂的语言结构和含义至关重要。

在设计大规模的Transformer模型时，通常会使用一个混合策略，结合局部和全局的稀疏模式，以及可能的全注意力层。这种混合使用策略帮助模型在保持处理速度的同时，也能有效地处理各种语言理解和生成任务。



# 参考

ChatGPT4
