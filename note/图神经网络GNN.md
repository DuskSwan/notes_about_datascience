**参考**

[A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)

**说明**

有关图的基础知识，在本文中的描述会比较粗略。需要的时候再做详细补充。

关于GNN，有的时候它表示一个完整的网络，这意味着它的输入是图片，输出是任务目标；有的时候它表示完整网络中的一个层，就好像CNN会代表卷积层一样。关于GNN的理解要结合上下文进行。

# 图基本知识

图由点（vertex，node）和边（edge，link）两种基本元素构成。

# 思路

图神经网络（Graph Neural Network），顾名思义是要跟图有关的。具体地说，有一些东西更适合用图作为数据结构来描述和操作，比如有机物分子或者社交网络，如果我们希望对结构为图的数据用上神经网络来进行一些计算，那就需要图神经网络。（典型网络比如MLP和CNN显然并不能直接处理一个图。）

对于一个图，可能的计算目标或者说任务目标有三种，就图整体而言的，就点而言的，和就边而言的，我们分别举例来解释这三种类型。假设我们的图是一个黑帮关系网络，每个节点是一个黑帮成员，每条边表示两个成员之间的关系亲密度，那么要预测这个黑帮整体的实力或者营收能力，就是一个整体而言的预测目标；要预测一个成员对老大是否忠诚或者他被抓进局子的概率，就是一个节点级别的预测目标；而对两个成员A和B，要预测他们起冲突的概率，就是一个边级别的预测目标。

接下来考虑怎么用模型实现从图到目标的变化。首先，考虑到机器学习中的样本通常是矩阵或者三维张量的形式，常见的网络层也都是针对这个形式设计的，我们自然希望把图也用这种形式表达。一个自然的想法是用图的邻接矩阵来代表图，但这样就会出现两个问题。第一，邻接矩阵通常是高阶而稀疏的，这意味着储存邻接矩阵会浪费大量内存空间。第二，在不同的节点顺序下会得到不同的邻接矩阵，而它们代表的是相同的图，模型处理后的结果也应该相同，但一个网络很难做到对所有这些长相不同的矩阵都得到同样的结果。

因此，GNN中应该首先包含一个“转化器”，转化器将图提炼成一个向量或是矩阵，然后再接分类头或是预测头来完成任务目标，就像NLP领域中把句子转化成编码以后接下游任务头一样。

概括地说，一个图经过GNN时发生了以下事情：GNN层使得图中节点和边的信息得到充分交流，将图变成一个具有对称不变性的东西（样貌上还是个图），然后通过池化层将边和点的信息集中起来，形成矩阵长相的特征集，最后就可以用常规的下游任务头来处理了。

# 网络结构

需要说明的是，GNN就像CNN一样，其概念有网络层和网络之分。CNN有时候代表的是CNN层，这个层并不是完整的神经网络；有时候代表CNN分类网络，这时候它是一个有完整功能的网络。GNN也一样，可以表示一个层，也可以表示一个功能完整的网络。接下来我们的叙述会明确究竟是GNN层还是GNN网络。

我们沿用参考blog的说法，采用的是Battaglia等人介绍的架构，在这个架构中，GNN指的是GNN层，其输入和输出都是图。而GNN网络是GNN层和下游任务头的结合。

### GNN层

用邻接表来唯一表示一张图G，其节点列表为$V=\{V_1,...,V_n\}$，其中的每个$V_i$是一个向量，代表该节点的特征；边列表为$E=\{E_1,...,E_m\}$，其中的每个$E_i$是一个向量，代表该边的特征；邻接表$\{(u^{(k)}_i,u^{(k)}_j)\}^m_{k=1}$中第$k$个元素表示第$k$条边是连接节点$u^{(k)}_i$和$u^{(k)}_j$的（有向图则是指向）；整体特征记录在向量$U$中。

![img](img/arch_independent.0efb8ae7.png)

在图G的每个特征，也就是$V_1,...,V_n,E_1,...,E_m$一直到$U$上分别使用一个单独MLP（不妨把这些MLP记为$f_{V_1},...,f_{V_n},f_{E_1},...,f_{E_m}$一直到$f_U$），每个特征都被更新了一次，但保持原本的邻接矩阵不变，这样图G就被更新了，而它还是一张图。这个过程如上图所示，注意图中的下标$n$表示第$n$层而不是第$n$个点/边。

同时施加在每个特征上的MLP们就组合成了一个GNN层，它的输入和输出都是图片，而且结构相同，只是特征数值不同。这样就是一个最朴素的GNN层了。就像全连接层一样，GNN层也可以堆叠很多次。

但是，这样并没有利用到节点之间的关联性，即便通过多次GNN层，节点之间也不会发生信息交换，一个节点无法从相邻节点处获得帮助判别的信息。自然地，要让节点发生联系，只需要先获取到邻接节点的特征然后和本身的特征结合，这种结合可以是相加、取均值等等。如果说之前描述的节点特征更新方式是$V'=f_V(V)$，那么一个更优秀的更新方式应该是
$$
V' = f_V\left(\sum_{V_i \in N(V)} V_i \right)
$$
其中$N(V)$是节点$V$的邻接点集。加入了点信息更新的GNN示意图如下。

![img](img/arch_gcn.40871750.png)

### 池化层

GNN层更新了图的特征，但我们的目的是预测任务目标，不妨假设是分类。一个简单的想法是，使用线形层作为分类头，预测目标是节点级别的就只输入节点特征，边级别的就只输入边特征，全局级别直接使用全局特征。然而，事情并不是这么简单，我们完全可能将图的信息存储在边特征中，而节点中没有信息；反过来也一样。所以，需要一种方法来从边收集信息并将其提供给节点，或是把节点的信息提供给边。这个方法就是池化。

> 举个例子，黑帮成员A对老大是否忠诚是一个节点属性，要确定这件事得看他和老大以及老大身边的人是否关系亲密，这又可以从他们会面的次数来判断，而会面是一个边级别的特征。

要让一个节点获取它周围边的信息分两步，先把周围边的信息聚合起来，这可以通过直接相加完成（当然也可以是求均值或是最大值），再把聚合后的信息与节点信息合并。

假设节点是$V$，其连接的边集合为$N(V)$，边的信息之和就是$E'=\displaystyle\sum_{E\in N(V)} E$，然后可以通过拼接后嵌入，也就是

$$
V'=MLP(V\|E')
$$

来更新$V$。也可以是先嵌入后直接求和，也即

$$
V'=V+MLP(E')
$$

总之，可以实现把边信息汇总到节点。

同理，也可以把节点信息汇总到边，或者是把节点信息汇总到整体。当然也可以把整体信息传递给一个点或者一条边。

### 有信息传递的GNN层

前文我们提到，在更新节点时应该利用邻接点的信息。想得再远一步，每一步更新当然也可以用上边和整体的信息。同理，更新边和整体时，也可以用上节点、边、整体的信息。总之，信息传递越充分，最后对一个点或者

这样一来每个GNN层中的更新都会变得很复杂，如下图。

![img](img/arch_graphnet.b229be6d.png)

当然，更新的顺序并不具有强制性，你可以先把点更新到边，也可以先把边更新到点，这就是设计网络的问题了，得根据数据的特征来决定。

这个“有信息传递的GNN层”才应该是真正的GNN层，再加上池化层和下游任务头，就可以完成预测任务了。
