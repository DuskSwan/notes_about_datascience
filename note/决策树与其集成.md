[toc]



# 决策树

## 思路

对于一些被划入$C$个类别的样本（其余特征则都是离散变量），我们希望按照一系列分类标准，将所有$C$个类别分开，这一系列分类标准呈现树的形状，每次判断后都走入一条分支，最终的“叶子结点”就是归属的类别。下图是一个例子：

![](img/决策树.jpeg)

假设每个样本$x_i=(x_{i1},...,x_{ip})$有$p$个特征，每个样本属于$k$个类别中的一种。最开始，全部样本同属于根节点。在每个节点$G$处（不妨设此节点处有$m$个样本），我们希望选择某个特征作为分类依据（比如，这个特征大于20划入一类，小于20划为另一类），将样本按照该特征的值分割为多个子节点，每个叶子都是一个分类结果。

为了衡量选择的优劣，需要考量节点的混乱度：每次分割都可能使得节点更混乱，穷尽所有可能的分类依据，使得混乱度增加最少的那个分类依据，就是好的分类依据。

> 所谓的“混乱”，指的是这样的概念：对于一个节点（样本空间），其中的样本（变量）可能会属于多个类别（样本点），如果所有样本都是同一类，那么任抽一个样本，它的类别归属是唯一的，归类结果确定；如果这些样本大多属于一类，剩下的零散分布在其它类中，那么任抽一个样本，我们可以说它大概率属于最多的那一类，归类结果有倾向；如果抽取的样本等概率地落入多个类别，那么归类的结果最混乱、最难以预测。显见，这三种局面的不确定程度是逐渐增大的。为了衡量结果的不确定程度，我们希望用某个函数$I(G)$来描述节点$G$的不确定性，用$I(G|X_j,f)$来描述选取第$j$特征、依分割标准$f$分割之后，新产生的子节点的不确定性总和，二者的差$I(G|X_j,f)-I(G)$就是“混乱度的增量”。
>
> 这种“不确定”，其含义与混乱、均匀、不纯同质；相对的则是确定、有序、不均、纯粹。我们的目标是将各类别区分开，所以越有序、越不混乱更好，也即混乱度增加越小越好。数学形式的函数会写在“细节”部分。

依次（此处应该可以有多种顺序，比如按照节点产生的顺序，按照树的前序遍历等等）按照上面的方式划分树的节点，直到满足某个标准（比如叶子结点总数够多，树的深度够大，叶子的样本量够小，叶子的纯度够高等等）才停止。现在，全部的数据可以用树状结构来储存，每个叶子节点中样本类别的众数，就是该叶子节点被判为的类别。对于一个新样本，只要判断它应该属于哪个叶子节点，就能判别它的类别。

## 细节与改进

### 1、不确定性的度量函数

假设在节点$G$处，有$m$个样本，分别属于$C$个类别$c_1,c_2,...c_C$。每个样本有$p$个特征，用$X_1,X_2,...,X_p$来表示。对于第$j$个特征，它有$m_j$个可能的取值，对于连续型特征，样本取值通常不会重复，$m_j$通常达到$m$；而对于离散型变量，其类别数通常小于样本数，也即$m_j<m$。将$X_j$可以取的值记为$x_1^{(j)},x_2^{(j)},...,x_{m_j}^{(j)}$。

用$p(c_i|G)$记录属于$G$处属于类$c_i$的样本占$G$ 处总样本数的比例（频率），这实际上是对$x\in c_i$这件事的发生率的估计。显然有$\sum\limits_{i=1}^C p(c_i|G)=1$。

有以下几种方法定义节点划分前后的”不确定性“（或者叫不纯度、混乱度、均匀度）：

（1）信息熵

用
$$
\begin{align}
I(G) = -\sum_{i=1}^{C} p(c_i|G)\log_2(p(c_i|G)) \\
\end{align}
$$
衡量节点$G$分割前的不确定性，称为（信息）熵不纯度。这实际上就是用频率替换概率代入了信息熵的公式。

显见，当节点中的类别均匀分布，信息熵取最大值$\log_2C$；当节点中仅有一个类别，信息熵取最小值$0$。值得注意的是，式中默认了$0\log_20=0$，这实际上是一个极限过程。

假设依据特征$X_j$、按照某种规则$f$将$G$分割成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则它们的总混乱度用加权平均数
$$
I(G|X_j,f)
=\sum_{k=1}^K\frac{m_k}m I(G_k)
$$
来表示。用信息熵的下降值$ I(G)-I(G|X_j,f)$来衡量分割的好坏（可以证明分割一定会使信息熵下降），下降值越大，则分割后越有序，则越好。该下降值称为信息增益（InfoGain)。

实践中，信息增益倾向于将节点分割得很细、每个叶子节点都是单一类的（纯粹的），这会导致过拟合问题。对此的一个改进是，用“分裂信息量”来限制分裂出的节点数。

假设节点$G$分裂成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则定义分裂的信息量为
$$
\text{SplitInfo}=-\sum_{k=1}^K \frac{m_k}{m} \log_2 (\frac{m_k}{m})
$$
可以看出，$K$越大，这个值也会越大。用信息增益率
$$
\text{InfoGainRatio}
=\frac{\text{InfoGain}}{\text{SplitInfo}}
=\frac{ I(G)-\sum\limits_{k=1}^K\frac{m_k}m I(G_k)}{-\sum\limits_{k=1}^K \frac{m_k}{m} \log_2 (\frac{m_k}{m})}
$$
代替信息增益作为衡量分割优劣的依据，就能避免分割出过多的叶子结点。

（2）Gini指数

节点$G$处的Gini指数为
$$
I(G)=\operatorname{Gini}(G)
=\sum_{i=1}^C p(c_i|G) \big(1-p(c_i|G)\big)
=1-\sum_{i=1}^C \left[p(c_i|G)\right]^2
$$
显见，当节点中的类别均匀分布，Gini指数取最大值$1-\frac1C$；当节点中仅有一个类别，Gini指数取最小值$0$。

假设依据特征$X_j$、按照某种规则$f$将$G$分割成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则它们的总混乱度用加权平均数
$$
I(G|X_j,f)
=\operatorname{Gini}_{\text{split}}(G)
=\sum_{k=1}^K\frac{m_k}m \text{Gini}(G_k)
$$
来表示。由于Gini指数是小于$1$的，加权平均数一定会比原来更小，所以变化量是负值。我们于是用Gini指数的下降值$ I(G)-I(G|X_j,f)$来衡量分割的好坏，下降值越大，则分割后越有序，则越好。

（3）误分率

节点$G$处的误分率定义为
$$
I(G)=\text{Error}(G)
=1-\max\limits_{1\leqslant i\leqslant C} \{ p(c_i|G) \}
$$
可见，一个节点中如果某一类占的比重很大，误分率就会比较小；如果各个类比较均匀，误分率就比较大。所以这也是衡量混乱度的指标。

假设依据特征$X_j$、按照某种规则$f$将$G$分割成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则它们的总混乱度用加权平均数
$$
I(G|X_j,f)
=\operatorname{Error}_{\text{split}}(G)
=\sum_{k=1}^K\frac{m_k}m \text{Error}(G_k)
$$
来表示。同理，一个节点被拆分后，混乱度下降越多越好。

（4）方差

前述的度量都针对离散型目标（这是自然的，因为决策树的目的就是分类）。但实际上，即使目标是连续的，我们也可以通过划分连续变量的分布区间来使之离散化，用一个代表性的值（比如均值）来作为这个区间所有样本的目标特征（标签、响应变量）。从这个角度看，决策树也可以用于做回归，此时可以称之为回归树。

针对连续型目标，无法使用前述的度量，所以我们考虑用方差衡量节点分裂前后的混乱度。

具体来说，假设在节点$G$处，有$m$个样本，它们的目标特征分别是$y_1,y_2,...,y_m$（一般来说不会出现重复）。那么节点$G$处的混乱程度就是$(y_1,y_2,...,y_m)$的方差$I(G)=V(y)=\displaystyle\frac{1}{m-1}\sum_{i=1}^m(y_i-\bar y)^2$。

针对连续型目标，应该选择一个特征的一个取值作为阈值，将节点分裂成两个子节点。分裂后的“总方差”是与上同理的加权平均值还是直接加和？暂时不知。我认为两种方式的效果应该是相同的。总之，分裂之后的“总方差”应该小于分裂前大方差，减小值越多，则这个分裂点选择越优。

### 2、划分规则

对一个节点$G$，选定特征$X_j$，如何选择哪些样本进入哪些子节点、总共多少个子节点？划分的具体规则到底是什么呢？

最初提出的决策树算法ID3（Iterative Dichotomiser 3）只能处理离散型特征与离散型标签（目标、响应变量）。选定特征$X_j$（它有$m_j$个可能的取值）之后，样本们的$X_j$取值为$x_1^{(j)},x_2^{(j)},...x_{m_j}^{(j)}$的各自划入一个子节点，共$m_j$个子节点。使用信息熵来度量混乱度。

接下来出现了算法C4.5，它在面对离散型特征时，依然采取如上的“一值一类”的拆分规则；在面对连续型特征时，则从$X_j$可能取到的值中选一个作为分裂阈值，如果选了$x_i^{(j)}$，则将$X_j\leqslant x_i^{(j)}$的样本归入一个节点，其余的$X_j > x_i^{(j)}$的样本放入另一个节点，仅有两个子节点。现在，C4.5既可以处理离散型特征，又可以处理连续型特征了，但是目标依然只能是离散的（分类变量）。此外，C4.5还用信息增益率代替了信息增益来衡量分裂的优劣。

新的方案是CART（Classification And Regression Tree），不论是按照离散型特征还是连续型特征，它都只分裂为两个节点。连续型特征的处理方案与C4.5相同；针对离散型特征，则只分为$X_j= x_i^{(j)}$和$X_j\neq x_i^{(j)}$这两组。此外，CART针对离散型目标使用Gini指数，针对连续型目标使用方差，来度量混乱度。

### 3、优化方法-剪枝

为了避免决策树太过复杂，产生过拟合，我们希望减除一些不必要的分裂。主要有预剪枝和后剪枝两种思路。

预剪枝会设定一个“混乱度下降阈值”，如果某个节点在分裂后的混乱度下降值不超过阈值，则认为这个分裂没必要进行，直接将该节点视为叶子结点即可。

后剪枝通过指标——整体损失函数，来衡量树的优劣。越小的整体损失意味着树越好。先建立完整的树，考虑是否要进行一些剪枝操作。如果剪枝之后的整体损失函数变小，就说明这次剪枝可取，否则不可取。遍历全部的剪枝方案，最终可以让整体损失函数达到（局部）最小。

假设一棵树$T$共有$|T|$个节点，记为$G_1,G_2,...,G_{|T|}$，它们所蕴含的样本数分别是$n_1,n_2,...,n_{|T|}$，选定某种混乱度度量函数$I(G)$，那么这棵树的整体损失函数（也可称复杂性代价）定义为
$$
R_\alpha(T)=\text{Cost}_\alpha(T)
=\sum_{t=1}^{|T|} n_t I(G_t) +\alpha|T|
$$
其中的$\alpha$是复杂度控制参数，$\alpha$越大，越倾向于选择节点数少的树。

## 代码

1、分类树

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier

```python
#签名
class sklearn.tree.DecisionTreeClassifier(*, 
	criterion='gini', 
	splitter='best', 
	max_depth=None, 
	min_samples_split=2, 
	min_samples_leaf=1, 
	min_weight_fraction_leaf=0.0, 
	max_features=None, 
	random_state=None, 
	max_leaf_nodes=None, 
	min_impurity_decrease=0.0, 
	class_weight=None, 
	ccp_alpha=0.0)
#样例
from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
```



超参数：

​	criterion：不纯度度量。默认'gini'即使用Gini指数度量。还可以选择'entropy'即信息熵。

​	max_depth：树的最大深度。如果不指定，则会一直计算直到叶子节点全部纯净、或者内部样本量足够小。

​	min_samples_leaf：叶子节点的最小样本数。如果是整数则直接指定；如果是小数，则视为是占全体样本的比例（也即用`ceil(min_samples_leaf * n_samples)`作为最小样本数）

​	min_impurity_decrease：不纯度下降的最大值。用于在预剪枝中筛选优秀子树。

​	ccp_alpha：整体损失函数的参数$\alpha$。这一参数需要sklearn版本0.22才有。

属性：

方法：

​	fit(X,Y)：训练。其中X应该是记录了特征的二维数组，Y是记录了标签的一维数组。

其他操作：

```python
#绘制分类树
>>> tree.plot_tree(clf) 

#以文本形式输出树
>>> r = export_text(clf, feature_names=df['feature_names'])
>>> print(r)
|--- petal width (cm) <= 0.80
|   |--- class: 0
|--- petal width (cm) >  0.80
|   |--- petal width (cm) <= 1.75
|   |   |--- class: 1
|   |--- petal width (cm) >  1.75
|   |   |--- class: 2
```

2、回归树

签名：

```Python
class sklearn.tree.DecisionTreeClassifier(criterion='mse'
                                          ,splitter="random"
                                          ,max_depth=None
                                          ,min_samples_split=2
                                          ,min_samples_leaf=1
                                          ,min.weight_fracton_leaf=0.0
                                          ,mac_features=None
                                          ,random_state=None
                                          ,max_leaf_nodes=None
                                          ,min_impurity_decrease=0.0
                                          ,min_impurity_split=None
                                          ,presort=False
                                         )

```

超参数：

（大多数都和决策树含义相同）

​	criterion：回归树衡量分枝质量的指标，支持的标准有三种。"mse"使用均方误差；"friedman_mse"使用费尔德曼均方误差，这种指标是用费里德曼对潜在分支中的问题改进后的均方误差；输入 "mae"使用绝对平均误差MAE（mean absolute error）

方法：

​	fit(X,Y)：训练。其中X应该是记录了特征的二维数组，Y是记录了标签的一维数组。

​	score(*X*, *y*)：针对测试集X、和真实的标签y，计算判定系数。

​	

# 随机森林

## 思路

如果只有一棵决策树，难免出现过拟合或者欠拟合的情况，结果的偏差在所难免。但如果有很多棵相互无关的决策树，各自独立地做出判断，投票得出判别结果，就大大降低了出差错的风险。这样由多棵树共同组成的分类模型，就成为森林。而为了让决策树之间互不相关，每棵树只随机抽取一部分样本来建立，于是称为随机森林。在实践中，也可以建立在其他各种分类器上（而不是必须依赖决策树）。

## 代码

1、随机森林分类器

在该随机森林中，使用决策树作为基本分类单元。

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=forest#sklearn.ensemble.RandomForestClassifier

签名：

```python
class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, 
                                              criterion='gini', 
                                              max_depth=None, 
                                              min_samples_split=2, 
                                              min_samples_leaf=1, 
                                              min_weight_fraction_leaf=0.0, 
                                              max_features='auto', 
                                              max_leaf_nodes=None, 
                                              min_impurity_decrease=0.0, 
                                              bootstrap=True, 
                                              oob_score=False, 
                                              n_jobs=None, 
                                              random_state=None, 
                                              verbose=0, 
                                              warm_start=False, 
                                              class_weight=None, 
                                              ccp_alpha=0.0, 
                                              max_samples=None)
```

超参数：

​	n_estimators：森林中决策树的数量。

​	criterion：不纯度度量。可以是Gini指数'gini'或者信息熵'entropy'。

​	max_depth：决策树的最大深度。

​	max_features：建立每棵树所采用的最大特征值数。传入整数直接表明数量；浮点数表示占全部特征的比例；None表示使用全部特征；“auto”等价于“sqrt”表示使用全部特征数的平方根；“log2”表示使用全部特征数的以二为底的对数。

​	min_samples_split：待分裂节点的最小样本数。样本数小于该节点的不会再分裂。

​	min_samples_leaf：决策树的叶子节点的最小样本数。如果是整数则直接指定；如果是小数，则视为是占全体样本的比例（也即用`ceil(min_samples_leaf * n_samples)`作为最小样本数）

​	max_leaf_nodes：决策树的叶子节点的最大数目。用于在后剪枝中控制树的复杂度。

​	min_impurity_decrease：不纯度下降的最大值。用于在预剪枝中筛选优秀子树。

​	bootstrap：是否抽样。如果是，则每次抽样建立决策树；如果否，则每次用全部数据建立决策树。

​	oob_score：仅在需要抽样（bootstrap=True）时生效。是否要使用抽样样本之外的样本评估泛化得分。

​	random_state：在涉及的随机函数中充当随机种子。如果传入一个整型，就拿它当做随机种子。此外也可以传入一个numpy.random.RandomState对象。

​	ccp_alpha：整体损失函数的参数$\alpha$。这一参数需要sklearn版本0.22才有。

​	max_samples：仅在需要抽样（bootstrap=True）时生效。设定每棵树在抽样时的样本数。默认等于全部样本数；传入整数则直接使用；传入小数则视为占全部样本数的比例。这一参数需要sklearn版本0.22才有。



属性：

​	estimators_：由树组成的列表。每棵树是一个DecisionTreeClassifier对象。

​	classes_：目标特征。也即类别组成的数组。

​	oob_score_：该属性仅在oob_score=True时产生。是使用抽样样本之外的样本估计获得的训练数据集的分数。



方法：

​	fit(*X*, *y*)：根据样本训练模型。

​	apply(*X*)：针对数据集X，返回每棵树对每个样本的拟合结果。以(n_samples, n_estimators)形式的数组给出。

​	predict(*X*)：针对数据集X进行预测。

​	predict_proba(*X*)：针对数据集X，返回每个样本属于各个类别的概率。以(n_samples, n_classes)形式的数组给出。

​	score(*X*, *y*)：针对测试集X、和真实的标签y，计算预测的平均正确率。



# GBDT

GBDT（Gradient Boosting Decision Tree，梯度提升决策树）是一种集成学习方法。它的核心思想是使用梯度提升算法（Gradient Boosting），通过迭代地优化损失函数，来逐步改进模型的预测效果。

梯度提升算法的基本思想是逐步构建模型，每一步都在减少模型的损失函数。具体到GBDT，它每次迭代都会添加一个决策树，尝试拟合上一棵（准确地说是一群）决策树的损失函数负梯度，以降低总体损失函数。这个过程就像是在进行梯度下降，不过这里的"梯度"是通过构建新的树来近似的。

## 过程

假设有$n$个样本，记为 $\{(x_i, y_i)\}, i = 1, \dots, n$，GBDT的计算过程可以详细分解为以下几个步骤：

---

1.**初始化模型**

GBDT从一个简单的常数模型开始，希望模型得到的结果是一个常数：

$$
F_0(x) = \arg \min_c \sum_{i=1}^n L(y_i, c)
$$

在回归任务中，通常将 $F_0(x)$ 初始化为目标值 $y$ 的平均值：

$$
F_0(x) = \frac{1}{n} \sum_{i=1}^n y_i
$$

2.**迭代训练决策树**

GBDT逐步构建模型，每一轮（即每一棵新树）都拟合当前模型的损失函数负梯度（对均方损失来说恰是残差），第 $m$ 轮迭代为：

（1）计算负梯度（残差）： 

对每个样本，计算当前模型预测值与真实值之间的误差（损失函数的负梯度），得到每个样本的残差：
$$
r_i^m = -\left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x) = F_{m-1}(x)}
$$

> 为什么不直接最小化预测值和真实值的误差，而是用损失函数的负梯度？事实上，损失函数的梯度才是误差的“通用”衡量方法，绝对误差可以看作一个特殊的梯度。而如果损失函数是凸的，只有在预测值等于真实值时梯度才会为0，所以优化损失函数梯度是可行的。

对于均方误差（MSE）损失函数 $L(y, F(x)) = \frac{1}{2} (y - F(x))^2$，负梯度等于实际值和预测值之差：
$$
r_i^m = y_i - F_{m-1}(x_i)
$$
对于其他损失函数，如对数损失等，残差计算会有所不同，但本质上都是通过负梯度来表示残差。

（2）拟合残差（训练新的决策树）：

使用上一轮计算的残差 $r_i^m$ 作为目标值，拟合一个新的决策树 $h_m(x)$，此决策树的任务是尽可能精确地预测这些残差$h_m(x) \approx r_i^m$。

（3）更新模型： 

将当前模型更新为之前模型和新树的组合，更新公式如下：
$$
F_m(x) = F_{m-1}(x) + \alpha h_m(x)
$$
其中，$\alpha$ 是学习率（通常设置为较小值，如 0.1 或更小），控制每棵树对模型的贡献，防止模型过拟合。

> 这里也可以看出，$h_m(x)$拟合的对象是损失函数负梯度而不是简单的预测绝对误差。如果是后者，应该直接加进去就好了，而拟合负梯度时才需要乘学习率，$\alpha h_m$加进$F_{m-1}$可以看作是自变量$F_{m-1}(x)$向着负梯度方向$f_m(x)$移动了一步。

（4）重复迭代： 

重复上述步骤，继续构建决策树 $h_1(x), h_2(x), \dots, h_M(x)$，直到达到设定的树的数量 $M$ 或模型误差收敛。

3.**GBDT的最终模型**

经过 $M$ 轮迭代后，GBDT模型的最终输出为所有基学习器的加权和：

$$
F(x) = F_M(x) = F_0(x) + \sum_{m=1}^M \alpha h_m(x)
$$

这个模型将每一棵树的预测结果进行累加，从而形成一个强大的预测模型。



# XGBoost

XGBoost可以看作是GBDT的改进版本，它对GBDT算法进行了许多优化。XGBoost的全称是“eXtreme Gradient Boosting”，其主要优化包括支持分布式计算、使用贪心算法进行节点分裂、添加L1和L2正则化、对并行计算的支持等。这些优化显著提升了模型的速度、准确性和稳定性。

以下只解释XGBoost相对GBDT改进的地方，不再赘述全流程。

## 改进

### 损失函数正则化

XGBoost的损失函数为
$$
L(y,\hat y) = \sum_{i=1}^n l(y_i,\hat y_i) + \sum_{k=1}^K \Omega(f_k)
$$
其中$y_i$和$\hat y_i$分别是每个样本的真实值和预测值，$f_k$是依次建立的一系列决策树。$\Omega(f)$就是正则化项，它实际上描述了一棵决策树$f$的复杂程度。

假设决策树$f$总共有$T$片叶子，每片叶子有一个权重值 $w_j \in R$，那么正则化项可以表示为
$$
\Omega(f)=\gamma T+\lambda \sum_{j=1}^T w_j^2
$$
其中$\gamma$和$\lambda$是系数。叶子节点的权重是什么意思，如何计算呢？待补充




# LightGBM

LightGBM是由微软开发的一种基于GBDT的优化模型，主要用于处理大规模数据。它使用基于直方图的分割方式，分割数据更加快速、内存占用更低，尤其适用于大数据集和高维稀疏数据。LightGBM引入了基于叶子（leafwise）的生长策略，与XGBoost的基于层的生长策略不同，使其能够捕获更复杂的模式。

# 参考

解释XGBoost：https://blog.csdn.net/v_JULY_v/article/details/81410574