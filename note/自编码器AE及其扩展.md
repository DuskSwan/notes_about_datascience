（本文内容在AI帮助下完成）

参考：

提供图片：https://zhuanlan.zhihu.com/p/133207206

提供了比较清晰的思路描述：https://keras-cn.readthedocs.io/en/latest/legacy/blog/autoencoder/

提供多种多样的AE，同时VAE的结构图不错：https://github.com/zangzelin/Auto-encoder-AE-SAE-DAE-CAE-DAE-with-keras-in-Mnist-and-report/blob/master/Report.md#25-%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8variational-auto-encoders-vaekingma-2014

# 自编码器AE（Auto Encoder）

自编码器是一种通过无监督学习来寻找数据压缩表示的神经网络。其目标是学习一种能够表示输入数据的高效编码，同时这种编码能够被用来重构回与原始数据相似的数据。

自编码器的核心目标是**数据表示的学习**。这意味着网络试图找到一种方法，以较小的数据形式（编码）准确地表示输入数据。这个过程中，自编码器学习到的编码通常比输入数据的维度要小，这有助于提取数据的内在特征，并且有助于数据的压缩和去噪。配合适当的维度和稀疏约束，自编码器可以学习到比PCA等技术更有意思的数据投影。通过这种方式，自编码器可以被用于：

- **数据降维**：转换高维数据到低维空间，方便数据可视化和处理。
- **特征提取**：学习数据的有用特征表示，为其他机器学习任务服务。
- **去噪**：学习从损坏的输入数据中重构原始数据。
- **数据生成**：在某些情况下，自编码器可以生成新的数据实例，特别是变分自编码器（VAE）。

自编码器由两部分组成：**编码器**和**解码器**。

- **编码器**：这部分的任务是将输入数据转换成一个压缩的内部表示，即编码。编码器将高维的输入数据映射到一个低维的隐藏层。
- **解码器**：解码器的任务是将编码重新转换（或重构）成原始输入数据。理想情况下，解码后的输出与原始输入非常相似。

以一个最简单的自编码器为例，其编码和解码器都是线性层。实际中的编码/解码器结构可能复杂得多。

![image-20240330155542319](D:\GithubRepos\notes_about_datascience\note\img\image-20240330155542319.png)

将一个样本的输入记为$x$，隐藏层/编码记为$h$（维数通常小于输入），输出记为$y$。使用输出$y$（即重构的输入）与原始输入$x$来计算损失（loss），但我们真正感兴趣的是隐藏层（编码层）$h$中的表示，它提供了输入数据的一个压缩表示。

自编码器的训练过程目的是使输出$y$尽可能接近输入$x$，从而保证隐藏层$h$中的压缩表示保留了输入数据的关键信息。换句话说，我们通过最小化$x$和$y$之间的差异来间接地优化$h$，使得$h$成为一个有效的数据表示。

常用的损失函数包括均方误差MSE和二元交叉熵BCE。

均方误差是衡量重构值与原始值之间差异的常见方法，特别适合实值数据。其公式表示为：

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{x}_i)^2
$$

其中，$x_i$ 是原始数据点，$\hat{x}_i$ 是对应的重构数据点，而 $n$ 是数据点的总数。

特别地，当输入数据被归一化到 $[0, 1]$ 区间内时，二元交叉熵成为一个适合的选择。它是这样计算的：

$$
BCE = -\frac{1}{n} \sum_{i=1}^{n} [x_i \log(\hat{x}_i) + (1 - x_i) \log(1 - \hat{x}_i)]
$$

在这里，$x_i$ 同样代表原始数据点，$\hat{x}_i$ 是重构后的数据点，$n$ 是数据点的总数。

# 变分自编码器VAE（Variational autoencoder）

变分自编码器（VAE）是一种生成模型，生成模型的目标就是学习得到一个分布P(X)，使得该分布和数据的真是分布P(X)很接近，这样一来，我们就可以根据得到的P(X)来生成该数据集中到数据，也就是达到了生成数据的目的，这就是生成模型的最终目标。

VAE通过自编码器的框架实现，但在标准自编码器（AE）的基础上引入了概率分布的概念。VAE的目的不仅仅是学习输入数据的有效表示，还在于能够生成新的数据点，这些数据点与训练数据具有相同的分布。

## 思路

变分自编码器（VAE）是一种基于概率生成模型的深度学习技术，其核心思想是通过学习输入数据的潜在（隐）变量的分布来生成新的数据点。VAE的实现依赖于隐变量模型和变分推断的框架。

VAE假设存在一个隐变量空间（latent space），其中的隐变量$Z$能够生成观察到的数据点$X$，并且$Z$可以从某个简单的分布（高斯分布）中采样得到。模型的目标是学习这个潜在空间的结构，以便我们可以从中采样并通过解码器生成新的数据点。

根据贝叶斯公式，有
$$
P(X)=P(Z)P(X|Z)
$$
我们假设$Z\sim N(0,I)$，现在只要知道$P(X|Z)$就好了，但是怎么才能知道呢？

考虑到$P(Z)=P(X)P(Z|X)$，我们要是能让一个模块A根据$X$算出$Z$（的条件分布），它就相当于$P(Z|X)$了。对于一个已知的$X$，让模块A计算出$Z$（的条件分布），用“模块A计算出的$Z$的分布”与“标准正态分布”的差距作为损失函数去训练模块A，就能得到$P(Z|X)$。

> 特别说明一下，条件分布$P(Z|X)$和分布$P(Z)$实质上并不相同，毕竟$P(Z|X)$会蕴含$X$的信息。但站在编码器的角度，是希望它们尽可能相似的，因为我们在训练解码器时实质上是用条件随机变量$Z|X$来代替随机变量$Z$，只有当二者的分布相近时这个代替才有效。
>
> 说到这又引出了另一个问题，编码器按说是P(Z∣X)的估计器，而条件分布P(Z∣X)和Z的分布N(0, I)实质上是不一样的，如果它要估计的准确，就不会等于N(0, I)，如果要等于N(0, I)，就不会估计的准确，这不是自相矛盾吗？
>
> 事实上，这涉及到在模型的表示能力和潜在空间的规则化之间寻找平衡（GPT原话）。换言之，编码器作为P(Z∣X)的估计器并不那么准确，也和N(0, I)并不那么接近，它是一个平衡的产物，最终只要能生成可靠的样本就好了，不需要为了它估计不准确或者不接近N(0, I)而苦恼。而这个平衡是由损失函数的构造来实现的。

一旦有了$P(Z|X)$（对任意$X$），我们对$X$计算出对应的$Z$，这时要是能让另一个模块B根据$Z$再算出$X$，那模块B就相当于$P(X|Z)$了呀！对于一个已知的$X$和对应的$Z$，让模块B根据$Z$计算出$X'$，用$X$与$X'$的差距作为损失函数去训练模块B，就能得到$P(X|Z)$，大功告成！

仔细想想，模块A就是个编码器，根据样本找到隐变量（的条件分布），模块B就是个解码器，根据隐变量样本计算或者说采样出生成样本X'。所以我们梦想中的生成式模型，意外地可以由自编码器的结构产生。

> 解码器到底是在计算还是在采样？让我们看看GPT怎么说，我觉得这段话一个字都不用改。
>
> 实际上，这里存在两种理解方式，它们取决于我们如何视角化解码器的操作，以及在实践中如何实现这一过程。
>
> 解码器作为确定性函数：在许多VAE实现中，解码器确实是作为一个确定性函数来操作的：给定一个潜在变量Z，解码器产生一个确定性的输出X′。在这种情况下，解码器可以被看作是定义了P(X∣Z)的参数化模型，其中X′代表了P(X∣Z)的均值或是最可能的状态（对于连续或离散输出）。因此，虽然解码器的操作是确定性的，但它通过与一定的损失函数（如均方误差或交叉熵）结合，隐含地定义了一个条件概率分布。
>
> 解码器作为概率采样过程：另一种理解方式是，即使在实践中解码器输出一个确定性的X′，这个过程也可以被视为从分布P(X∣Z)中“隐含地”采样。在这个视角下，即便解码器为每个Z生成一个固定的X′，这个生成过程本身代表了从由Z参数化的分布中抽取样本的操作。这种理解方式更加强调了VAE作为整体的概率性质，即整个生成过程（从Z到X′）被视为概率模型的一部分。



## 模型搭建与训练

VAE的网络结构如下（两张图各有优点，都扔上来），我们考虑最初也是最简单的情况，$Z$服从的是标准正态分布。

![image-20240411204510884](D:\GithubRepos\notes_about_datascience\note\img\image-20240411204510884.png)

![image-20240330162223594](D:\GithubRepos\notes_about_datascience\note\img\image-20240330162223594.png)

过程是：

1. 编码：首先将输入数据 $x$ 映射到潜在变量 $z$ 的分布上，通常是高斯分布的参数（均值$\mu$和方差$\sigma^2$）。
2. 采样：从这个分布中采样潜在变量 $z$。由于直接从分布中采样的操作无法进行梯度传递，VAE采用了“重参数化技巧”来解决这个问题。
3. 解码：然后将潜在变量 $z$ 映射回数据空间，尝试重构输入数据 $x$。


### 

第一部分是编码器网络。输入是数据点 $x$，输出是描述潜在变量 $z$ 分布的参数。对于高斯分布，输出是均值 $\mu$ 和对数方差 $\log(\sigma^2)$）。
$$
q_\phi(z|x) = \mathcal{N}(z; \mu(x), \sigma^2(x))
$$

其中，$\phi$ 是编码器网络的参数。上式想要表达编码器相当于一个后验的$z$的分布，但从数学上来看，它极为不严谨，等号左边是一个函数或者说概率，右边却是一个分布，这实在是难以理解。我认为$q_\phi(x) = (\mu(x), \sigma^2(x))$是更好的表达。编码器网络并不需要很复杂，在原论文中，由多个全连接层构成。

接下来要用到重参数化技巧。为了能够通过反向传播训练网络，VAE使用了重参数化技巧。具体来说，我们不直接从$\mathcal{N}(\mu(x), \sigma^2(x))$中采样出$z$，而是考虑从从标准正态分布 $\mathcal{N}(0,1)$ 中采样出$\epsilon$，然后计算

$$
z = \mu + \sigma \cdot \epsilon
$$

这样得到的$z$就服从$\mathcal{N}(\mu, \sigma^2)$。而且这样的$z$关于编码器网络的结果$\mu$与$\sigma^2$可导。

得到潜变量$z$后，输入解码器网络。输入是潜在变量 $z$，输出是重构的数据 $\hat{x}$。目标是最小化重构误差，即输入 $x$ 和重构 $\hat{x}$ 之间的差异。原论文中解码器同样采用了全连接层的结构。

接下来计算损失，损失包括两个部分，一个是重构损失函数，该函数要求解码出来的样本与输入的样本相似（与之前的自编码器相同），第二项损失函数是学习到的隐分布与先验分布的KL距离，作为一个正则项，它对学习符合要求的隐空间和防止过拟合有帮助。

重构损失（Reconstruction Loss）衡量的是重构数据与原始数据之间的相似度。它可以通过不同的方式计算，常见的有均方误差（MSE）和二元交叉熵（Binary Cross-Entropy，BCE）。和AE是一样的，不再赘述。

KL散度（KL Divergence）则衡量编码的潜在变量分布$q_\phi(z|x)$与先验分布$p(z)$之间的差异。在VAE中，先验分布通常假设为标准正态分布$\mathcal{N}(0, I)$。其计算公式为

$$
L_{\text{KL}} = KL[q_\phi(z|x) \,||\, p(z)] = \frac{1}{2} \sum_{j=1}^{J} (1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2)
$$
其中，$\mu_j$ 和 $\sigma_j^2$ 分别是编码得到的潜在变量 $z$ 的分布参数，$J$ 是潜在空间的维度。

VAE的总损失是重构损失和KL散度的组合，$L_{\text{total}} = L_{\text{recon}} + \beta L_{\text{KL}}$。这里的 $\beta$ 是一个超参数，用于平衡重构损失和KL散度。在一些变种中，$\beta$ 可以用来控制潜在空间的紧凑性和重构质量之间的权衡。

通过最小化总损失，VAE能够学习到能够有效重构原始数据同时保证潜在空间分布接近先验分布的参数。

### 模型推理

使用训练好的VAE生成新样本的过程，通常并不需要对训练集中的每个样本计算μ和σ2，然后求平均值。相反，生成新样本的过程通常是直接在潜在空间进行的：

- 从潜在空间的先验分布（通常是标准正态分布N(0,I)）中直接采样一个或多个点Z。因为训练过程中KL散度项鼓励潜在变量的分布接近先验分布，所以这种采样能够反映出经过训练的潜在空间的结构。
- 使用解码器将这些采样得到的潜在变量Z映射（解码）回数据空间，生成新的数据样本X′。

因此，在生成新样本时，你通常不会对已有样本的μ和σ2进行平均，而是直接在潜在空间进行采样。这样做的原因是，训练过程中的正则化（KL散度项）已经确保了潜在空间的结构是连续且有意义的，从而允许我们通过采样先验分布来生成多样化且有趣的新样本。
